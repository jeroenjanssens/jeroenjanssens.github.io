<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Jeroen Janssens</title>
		<description>Dutch Data Scientist, Teacher, Author, Entrepreneur</description>
		<link>http://www.jeroenjanssens.com</link>
		<atom:link href="http://www.jeroenjanssens.com/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Anomalies, concerts & data science at the command line [interview]</title>
				<description>&lt;p&gt;&lt;em&gt;This interview first appeared on &lt;a href=&quot;http://www.datascienceweekly.org/data-scientist-interviews/data-science-at-the-command-line-jeroen-janssens-interview&quot;&gt;Data Science Weekly&lt;/a&gt;&lt;/em&gt; in October 2014.&lt;/p&gt;

&lt;p&gt;
We recently caught up with Jeroen Janssens, author of &lt;a href=&quot;http://datascienceatthecommandline.com/&quot;&gt;Data Science at the Command Line&lt;/a&gt;. We were keen to learn more about his background, his recent work at YPlan and his work creating both the book and the (related) &lt;a href=&quot;http://datasciencetoolbox.org&quot;&gt;Data Science Toolbox&lt;/a&gt; project…
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Hi Jeroen, firstly thank you for the interview. Let's start with your background and how you became interested in working with data...&lt;/strong&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - What is your 30 second bio?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Howdy! My name is &lt;a href=&quot;https://twitter.com/jeroenhjanssens&quot;&gt;Jeroen&lt;/a&gt; and I'm a data scientist. At least I like to think that I am. As a Brooklynite who tries to turn dirty data into pretty plots and meaningful models using a MacBook, I do believe I match at least one of the many definitions of data scientist. Jokes aside, the first time I was given the title of data scientist was in January 2012, when I joined Visual Revenue in New York City. At the time, I was still finishing my Ph.D. in Machine Learning at Tilburg University in the Netherlands. In March 2013, Visual Revenue got acquired by Outbrain, where I stayed for eight months. The third and final startup in New York City where I was allowed to call myself data scientist was YPlan. And now, after a year of developing a recommendation engine for last-minute concerts, sporting events, and wine tastings, I'm excited to tell you that I'll soon be moving back to the Netherlands.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - How did you get interested in working with data? 
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - During my undergraduate at University College Maastricht, which is a liberal arts college in the Netherlands, I took a course in Machine Learning. The idea of teaching computers by feeding it data fascinated me. Once I graduated, I wanted to learn more about this excited field, so I continued with an M.Sc. in Artificial Intelligence at Maastricht University, which has a strong focus on Machine Learning.
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - So, what was the first data set you remember working with? What did you do with it?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - The very first data set was actually one that I created myself, albeit in quite a naughty way. In high school--I must have been fifteen--I managed to create a program in Visual Basic that imitated the lab computers' login screen. When a student tried to log in, an error message would pop up and the username and password would be saved to a file. So, by the end of the day, I had a &quot;data set&quot; of dozens of username/password combinations. Don't worry, I didn't use that data at all; this whole thing was really about the challenge of fooling fellow students. Of course I couldn't keep my mouth shut about this feat, which quickly led to the punishment I deserved: vacuum cleaning all the classrooms for a month. Yes, I'll never forget that data set.
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - I can imagine! Maybe it was that moment, though was there a specific &quot;aha&quot; moment when you realized the power of data?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Towards the end of my Ph.D., which focused on anomaly detection, I was looking into meta learning for one-class classifiers. In other words, I wanted to know whether it was possible to predict which one-class classifier would perform best on a new, unseen data set. Besides that, I also wanted to know which characteristics of that data set would be most important. 
&lt;br /&gt;
&lt;br /&gt;
To achieve this, I constructed a so-called meta data set, where its 36 features were characteristics of 255 &quot;regular&quot; data sets (for example, number of data points, dimensionality). I evaluated 19 different one-class classifiers on those 255 data sets. The challenge was then to train a meta classifier on that meta data set, with 19 AUC performance values as the labels.
&lt;br /&gt;
&lt;br /&gt;
Long story short, because I tried to do too many things at once, I ended up with way too much data to examine. For weeks, I was getting lost in my own data. Eventually I managed to succeed. The lesson I learned was that there's also a thing as too much data; not in the sense of space, but in density, if that makes sense. And more importantly, I also learned to think harder before simply starting a huge computational experiment!
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;br /&gt;
&lt;strong&gt;Makes sense! Thanks for sharing all that background. Let's switch gears and talk about this past year, where you've been the Senior Data Scientist at YPlan...&lt;/strong&gt; 
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - Firstly, what is YPlan? How would you describe it to someone not familiar with it?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Here's the pitch I've been using for the past year. YPlan is for people who want to go out either tonight or tomorrow, but don't yet know what to do. It's an app for your iPhone or Android phone that shows you a curated list of last-minute events: anything ranging from Broadway shows to bottomless brunches in Brooklyn. If you see something you like you can book it in two taps. You don't no need to go to a different website, fill out a form, and print out the tickets. Instead, you just show your phone at the door and have a great time! 
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - That's great! What do you find most exciting about working at the intersection of Data Science and entertainment?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - YPlan is essentially a market place between people and events. It's interesting to tinker with our data because a lot of it comes from people (which events do they look at and which one do they eventually book?). Plus, it's motivating trying to solve a (luxury) problem you have yourself, and then to get feedback from your customers. Another reason why YPlan was so great to work at, was that everybody has the same goal: making sure that our customers would find the perfect event and have a great time. You can improve on your recommendation system as much as you want (which I tried to do), but without great content and great customer support, you won't achieve this goal. I guess what I'm trying to say is that the best thing about YPlan were my colleagues, and that's what made it exciting.
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - So what have you been working on this year? What has been the most surprising insight you've found?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - At YPlan I've mostly been working on a content-based recommendation system, where the goal is essentially to predict the probability a customer would book a certain event. The reason the recommendation system is a content-based one rather than a collaborative one, is that our events have a very short shelf life, which is very different from say, the movies available on Netflix.
&lt;br /&gt;
&lt;br /&gt;
We've also created a backtesting system, which allows us to quickly evaluate the performance of the recommendation system to historical data whenever we make a change. Of course, such an evaluation does not give a definitive answer, so we always A/B test a new version with the current version. Still, being able to quickly make changes and evaluate has proved to be very useful. 
&lt;br /&gt;
&lt;br /&gt;
The most surprising insight is, I think, how wrong our instincts and assumptions can be. A recommendation system, or any machine learning algorithm in production for that matter, is not just the math you would find in textbooks. As soon as you apply it to the real world, a lot of (hidden) assumptions will be made. For example, the initial feature weighting I came up with, has recently been greatly improved using an Evolutionary Algorithm on top of the backtesting system. 
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;br /&gt;
&lt;strong&gt;Thanks for sharing all that detail - very interesting! Let's switch gears and talk about the book you've been working on that came out recently...&lt;/strong&gt; 
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - You just finished writing a book titled &lt;a href=&quot;http://datascienceatthecommand.com&quot;&gt;Data Science at the Command Line&lt;/a&gt;. What does the book cover?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Well, the main goal of the book is to teach why, how, and when the command line could be employed for data science. The book starts with explaining what the command line is and why it's such a powerful approach for working with data. At the end of the first chapter, we demonstrate the flexibility of the command line through an amusing example where we use The New York Times' API to infer when New York Fashion Week is happening. Then, after an introduction to the most important Unix concepts and tools, we demonstrate how to obtain data from sources such as relational databases, APIs, and Excel. Obtaining data is actually the first step of the OSEMN model, which is a very practical definition of data science by Hilary Mason and Chris Wiggins that forms the backbone of the book. The steps scrubbing, exploring, and modeling data are also covered in separate chapters. For the final step, interpreting data, a computer is of little use, let alone the command line. Besides those step chapters we also cover more general topics such as parallelizing pipelines and managing data workflows.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - Who is the book best suited for? 

&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - I'd say everybody who has an affinity with data! The command line can be intimidating at first, it was for me at least, so I made sure the book makes very little assumptions. I created a virtual machine that contains all the necessary software and data, so it doesn't matter whether readers are on Windows, OS X, or Linux. Some programming experience helps, because in Chapter 4 we look at how to create reusable command-line tools from existing Python and R code.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - What can readers hope to learn?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - The goal of the book is make the reader a more efficient and productive data scientist. It may surprise people that quite a few data science tasks, especially those related to obtaining and scrubbing, can be done much quicker on the command line than in a programming language. Of course, the command line has its limits, which means that you'd need to resort to a different approach. I don't use the command line for everything myself. It all depends on the task at hand whether I use the command line, IPython notebook, R, Go, D3 &amp; CoffeeScript, or simply pen &amp; paper. Knowing when to use which approach is important, and I'm convinced that there's a place for the command line.
&lt;br /&gt;
&lt;br /&gt;
One advantage of the command line is that it can easily be integrated with your existing data science workflow. On the one hand, you can often employ the command line from your own environment. IPython and R, for instance, allow you to run command-line tools and capture their output. On the other hand, you can turn your existing code into a reusable command-line tool. I'm convinced that being able to build up your own set of tools can make you a more efficient and productive data scientist.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - What has been your favorite part of writing the book?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Because the book discusses more than 80 command-line tools, many of which have very particular installation instructions, it would take the reader the better part of the day to get all set up. To prevent that, I wanted to create a virtual machine that would contain all the tools and data pre-installed, much like Matthew Russell had done for his book &lt;a href=&quot;http://miningthesocialweb.com&quot;&gt;Mining the Social Web&lt;/a&gt;. I figured that many authors would want to do something like that for their readers. The same holds for teachers and workshop instructors. They want their students up and running as quickly as possible. So, while I was writing my book, I started a project called the Data Science Toolbox, which was, and continues to be, a very interesting and educational experience.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - Got it! Let's talk more about the &lt;a href=&quot;http://datasciencetoolbox.org&quot;&gt;Data Science Toolbox&lt;/a&gt;. What is your objective for this project?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - On the one hand the goal of the Data Science Toolbox is to enable everybody to get started doing data science quickly. The base version contains both R and the Python scientific stack, currently the two most popular environments to do data science. (I still find it amazing that you can download a complete operating system with this software and have it up and running in a matter of minutes.) On the other hand, authors and teachers should be able to easily create custom software and data bundles for their readers and students. It's a shame to waste time on getting all the required software and date installed. When everybody's running the Data Science Toolbox, you know that you all have exactly the same environment and you can get straight to the good stuff: doing data science.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - What have you developed so far? And what is coming soon?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - Because the Data Science Toolbox stands on the shoulders of many giants: Ubuntu, Vagrant, VirtualBox, Ansible, Packer, and Amazon Web Services, not too much needed to be developed, honestly. Most work went into combining these technologies, creating a command-line tool for installing bundles, and making sure the Vagrant box and AWS AMIs stay up-to-date.
The success of the Data Science Toolbox is going to depend much more on the quantity and quality of bundles. In that sense it's really a community effort. Currently, there are a handful of bundles available. The most recent bundle is by Rob Doherty for his &lt;a href=&quot;https://generalassemb.ly/education/data-science&quot;&gt;Introduction to Data Science class at General Assembly&lt;/a&gt; in New York. There are a few interesting collaborations going on at the moment, which should result in more bundles soon. 
&lt;/p&gt;

&lt;p&gt;
&lt;br /&gt;
&lt;strong&gt;Thanks for sharing all the projects you've been working on - super interesting! Good luck with all your ongoing endeavors! Finally, let's talk a bit about the future and share some advice...&lt;/strong&gt; 
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - What does the future of Data Science look like?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - For me, and I hope for many others, data science will have a dark background and bright fixed-width characters. Seriously, the command line has been around for four decades and isn't going anywhere soon. Two concepts that make the command line so powerful are: working with streams of data and chaining computational blocks. Because the amount of data, and the demand to quickly extract value from it, will only increase, so will the importance of these two concepts. For example, only recently does R, thanks to magrittr and dplyr, support the piping of functions. Also streamtools, a very promising project from the New York Times R&amp;D lab, embeds these two concepts.
&lt;/p&gt;

&lt;p&gt;
&lt;strong&gt;Q&lt;/strong&gt; - One last question, you said you're going back to the Netherlands? What are your plans?
&lt;br /&gt;
&lt;strong&gt;A&lt;/strong&gt; - That's right, back to the land of tulips, windmills, bikes, hagelslag, and hopefully, some data science! About three years ago, when I was convincing my wife to come with me to New York City, the role of data scientist practically didn't exist in the Netherlands. While it still doesn't come close to say, London, San Francisco, or New York City, it's good to see that it's catching up. More and more startups are looking for data scientists. Also, as far as I'm aware, three data science research centers have been formed: one in Amsterdam, one in Leiden, and one in Eindhoven. These developments open up many possibilities. Joining a startup, forming a startup, teaching a class, consulting, training, research; I'm currently considering many things. Exciting times ahead!
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;br /&gt;
&lt;strong&gt;Jeroen&lt;/strong&gt; - Thank you ever so much for your time! Really enjoyed learning more about your background, your work at YPlan and both &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;your book&lt;/a&gt; and toolbox projects. Good luck with the move home! 
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;
Readers, thanks for joining us! If you want to read more from Jeroen he can be found on twitter &lt;a href=&quot;https://twitter.com/jeroenhjanssens&quot;&gt;@jeroenhjanssens&lt;/a&gt;.
&lt;/p&gt;
</description>
				<pubDate>Mon, 18 May 2015 14:00:00 +0200</pubDate>
				<link>http://www.jeroenjanssens.com/2015/05/18/interview-data-science-weekly.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2015/05/18/interview-data-science-weekly.html</guid>
			</item>
		
			<item>
				<title>IBash Notebook&#8253;</title>
				<description>&lt;p&gt;Did you know that there&amp;#39;s a Bash kernel for IPython Notebook?
It even displays inline images.
To give you a glimpse, the code cell below makes an API call to &lt;a href=&quot;http://memegenerator.net&quot;&gt;memegenerator.net&lt;/a&gt;, which generates images on demand. From the response, the URL of the generated image is extracted  using &lt;code&gt;jq&lt;/code&gt; and subsequently downloaded using &lt;code&gt;curl&lt;/code&gt;. The output is then displayed as an inline image by piping it to a function called &lt;code&gt;display&lt;/code&gt;. Perhaps a bit contrived, but if not with a meme, how else am I supposed to grab your attention these days?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ibash-notebook.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In this post, I first give some background on notebooks and the IPython Notebook/Jupyter project. Then, I explore the idea whether this &amp;quot;IBash Notebook&amp;quot; has the potential to become a convenient environment for doing data science. Subsequently, I explain how I added support for displaying inline images. As an aside, I wonder whether it would be feasible and worthwhile to publish my book &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;Data Science at the Command Line&lt;/a&gt; as a collection of notebooks. Finally, I discuss which issues remain to be improved and how you can try out IBash Notebook for yourself. I&amp;#39;m curious to hear what you think.&lt;/p&gt;

&lt;h3&gt;You get a notebook. And you get a notebook. Everybody gets a notebook!&lt;/h3&gt;

&lt;p&gt;Let&amp;#39;s take a step back for a moment. Doing research is hard. Recalling which steps you&amp;#39;ve taken, and why, is even harder. To be an effective researcher, you may want to &lt;a href=&quot;http://colinpurrington.com/tips/academic/labnotebooks&quot;&gt;keep a laboratory notebook&lt;/a&gt;. Besides having a record of your steps and results, this also allows you to improve reproducibility, share your research with others, and, yes, think more clearly. So, why wouldn&amp;#39;t you keep a notebook?&lt;/p&gt;

&lt;p&gt;Well, if you perform your research or analysis on a computer, where most steps boil down to running code, invoking commands, and clicking buttons, keeping an analogue notebook is rather cumbersome. Fortunately, since recently, digital counterparts are quickly gaining popularity. For the R community, for example, there&amp;#39;s &lt;a href=&quot;http://rmarkdown.rstudio.com&quot;&gt;R Markdown&lt;/a&gt;. And for those who use the Python scientific stack, there&amp;#39;s &lt;a href=&quot;http://ipython.org/notebook.html&quot;&gt;IPython Notebook&lt;/a&gt;. Both solutions are free and allow you to combine code, text, equations, and visualizations into a single document.&lt;/p&gt;

&lt;p&gt;The people behind the IPython project saw the potential of having a language-agnostic architecture. By creating a flexible messaging protocol, writing &lt;a href=&quot;http://ipython.org/ipython-doc/dev/development/kernels.html&quot;&gt;good documentation&lt;/a&gt; for it, and rebranding the project as the &lt;a href=&quot;http://jupyter.org/&quot;&gt;Jupyter project&lt;/a&gt;, they opened the door to other languages. And now, languages like Julia, Ruby, and Haskell have their own kernel. &lt;a href=&quot;http://beakernotebook.com/&quot;&gt;Beaker&lt;/a&gt;, a completely different project, even supports multiple languages in the same notebook.&lt;/p&gt;

&lt;h3&gt;What about poor old Bash?&lt;/h3&gt;

&lt;p&gt;To demonstrate how easy it is to create a new kernel for IPython Notebook, &lt;a href=&quot;https://twitter.com/takluyver/&quot;&gt;Thomas Kluyver&lt;/a&gt; created a Python package called &lt;a href=&quot;https://github.com/takluyver/bash_kernel&quot;&gt;bash_kernel&lt;/a&gt;. This Bash kernel basically works by using &lt;a href=&quot;https://pexpect.readthedocs.org/en/latest/&quot;&gt;pexpect&lt;/a&gt; to wrap around a Bash command line. When I stumbled upon this package I immediately got excited. This could be much more than just a demonstration. Call me crazy, but I believe that with some additional effort, we might have an IBash Notebook that would have some important advantages over a terminal (which is the standard environment to interact with the command line; see image below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/terminal.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;First, and perhaps most importantly, the command line is ad-hoc in nature, which makes it difficult to reproduce your steps or share them with your peers. To improve reproducibility, you could put those steps in a shell script, &lt;a href=&quot;http://www.gnu.org/software/make/&quot;&gt;Makefile&lt;/a&gt;, or &lt;a href=&quot;https://github.com/Factual/drake&quot;&gt;Drakefile&lt;/a&gt;, but when you&amp;#39;re working in a notebook, they would be stored automatically.&lt;/p&gt;

&lt;p&gt;Second, if you&amp;#39;re running a server or virtual machine, there would be no need to &lt;code&gt;ssh&lt;/code&gt; into it. As a result, Microsoft Windows users wouldn&amp;#39;t need to resort to a third-party tool like &lt;a href=&quot;http://www.chiark.greenend.org.uk/%7Esgtatham/putty/&quot;&gt;PuTTY&lt;/a&gt; anymore. I&amp;#39;m particularly interested in this advantage, together with the next two, because ever since I started writing &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;Data Science at the Command Line&lt;/a&gt;, I&amp;#39;ve been looking for ways to make the command line more accessible to newcomers.&lt;/p&gt;

&lt;p&gt;Third, for users who are new to the command line, a notebook with code cells could be less intimidating than a terminal with a prompt. Because the browser (and perhaps also IPython Notebook) is a familiar environment, the threshold to try out the command line will be lower.&lt;/p&gt;

&lt;p&gt;Fourth, in order to view an image located on a server or virtual machine, you normally have to go trough an extra hoop. Approaches that I know of are either: (1) copy this image to the host OS, (2) forward X11, or (3) serve it using, say, &lt;code&gt;python -m SimpleHTTPServer&lt;/code&gt; and then open it in a browser. With a notebook, images can be shown inline. Which brings us to...&lt;/p&gt;

&lt;h3&gt;Adding support for displaying inline images&lt;/h3&gt;

&lt;p&gt;For the Bash kernel to be a convenient environment for doing data science, it could use a few additional features besides running commands. Thanks to the architecture of IPython Notebook, inline Markdown and LaTeX equations work out of the box. Having seen &lt;a href=&quot;http://liftoffsoftware.com/Products/GateOne&quot;&gt;Gate One&lt;/a&gt; (a browser-based terminal that I had running on 200 EC2 instances for &lt;a href=&quot;http://strataconf.com/stratany2014/public/schedule/detail/36204&quot;&gt;my workshop at Strata NYC&lt;/a&gt;) and &lt;a href=&quot;https://pigshell.com&quot;&gt;pigshell&lt;/a&gt; (a shell-like website that lets you interact with various APIs as Unix files), which are both able to display inline images, I knew that&amp;#39;s what the Bash kernel needed next.&lt;/p&gt;

&lt;p&gt;I initially thought this would be as easy as detecting the MIME type of the output of a command. That way, when you would run &lt;code&gt;cat file.png&lt;/code&gt;, an image would be shown automatically. Unfortunately this approach didn&amp;#39;t work because, as I later learned, &lt;code&gt;pexpect&lt;/code&gt; isn&amp;#39;t meant to transfer binary data. With some suggestions from &lt;a href=&quot;https://twitter.com/takluyver/&quot;&gt;Thomas Kluyver&lt;/a&gt;, I implemented the following solution instead. (You may decide whether it&amp;#39;s a hack or not.) &lt;/p&gt;

&lt;p&gt;The solution includes a Bash function called &lt;code&gt;display&lt;/code&gt; that is registered when the kernel starts. That way, images can now be displayed by running something as simple as:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;display &amp;lt; file.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;or something as involved as:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;cat iris.csv &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;# Read our beloved Iris data set&lt;/span&gt;
cols -C species body tapkee -m pca &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Apply PCA using tapkee&lt;/span&gt;
header -r x,y,species &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;# Replace header of CSV&lt;/span&gt;
Rio-scatter x y species &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; display     &lt;span class=&quot;c&quot;&gt;# Create scatter plot using ggplot2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which produces:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/iris-pca.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;In case you&amp;#39;re interested, &lt;code&gt;cols&lt;/code&gt; and &lt;code&gt;body&lt;/code&gt; are used to only pass numerical columns and no header to &lt;a href=&quot;http://tapkee.lisitsyn.me/&quot;&gt;tapkee&lt;/a&gt;, which is a fantastic library for dimensionality reduction by &lt;a href=&quot;https://twitter.com/qdrgsm/&quot;&gt;Sergey Lisitsyn&lt;/a&gt;. These two Bash scripts, together with &lt;code&gt;header&lt;/code&gt; and &lt;code&gt;Rio-scatter&lt;/code&gt;, can be found in &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-at-the-command-line/tree/master/tools&quot;&gt;this repository&lt;/a&gt;. Speaking of command-line tools for plotting, &lt;a href=&quot;http://bokeh.pydata.org/&quot;&gt;Bokeh&lt;/a&gt;, which is a Python visualization library built on top of matplotlib, will soon have its own command-line tool as well.&lt;/p&gt;

&lt;p&gt;To see what the &lt;code&gt;display&lt;/code&gt; function looks like, we can run &lt;code&gt;type display&lt;/code&gt; in a notebook:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;display is a &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;
display &lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;TMPFILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;mktemp &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TMPDIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-/tmp&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/bash_kernel.XXXXXXXXXX&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    cat &amp;gt; &lt;span class=&quot;nv&quot;&gt;$TMPFILE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;bash_kernel: saved image data to: $TMPFILE&amp;quot;&lt;/span&gt; 1&amp;gt;&lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;2
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In words, &lt;code&gt;display&lt;/code&gt; saves the standard input to a temporary file and prints the filename to standard error. After a code cell has been evaluated, the Bash kernel simply extracts the filename from the output, detects its MIME type using the &lt;a href=&quot;https://docs.python.org/3.4/library/imghdr.html&quot;&gt;imghdr&lt;/a&gt; library, and sends the image data (encoded with base64) to the front end. Easy peasy.&lt;/p&gt;

&lt;p&gt;I chose the name &amp;quot;display&amp;quot; because there&amp;#39;s also a &lt;a href=&quot;http://www.imagemagick.org/script/display.php&quot;&gt;command-line tool in ImageMagick&lt;/a&gt; called &amp;quot;display&amp;quot; that accepts image data from standard input and shows it in a new window. Because that tool works only when X is running, I figured that a function called &amp;quot;display&amp;quot; could serve as a drop-in replacement when using IPython Notebook.&lt;/p&gt;

&lt;h3&gt;Aside: Publishing a book as a collection of notebooks&lt;/h3&gt;

&lt;p&gt;IPython Notebook can also be used to write entire books. &lt;a href=&quot;https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition&quot;&gt;Mining the Social Web&lt;/a&gt;, &lt;a href=&quot;https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers&quot;&gt;Probabilistic Programming and Bayesian Methods for Hackers&lt;/a&gt;, and &lt;a href=&quot;http://nbviewer.ipython.org/github/unpingco/Python-for-Signal-Processing/tree/master/&quot;&gt;Python for Signal Processing&lt;/a&gt; are but a few examples of books that have been published as a collection of notebooks (usually one notebook per chapter). The main advantage of a notebook as opposed to a book is that you can immediately run the code yourself. Instead of passively reading about a certain package or tool, you can actively try it out.&lt;/p&gt;

&lt;p&gt;I wonder if I could (and should) do the same with my book &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;Data Science at the Command Line&lt;/a&gt;. As an initial test, I manually converted part of the first chapter to a notebook, which you can &lt;a href=&quot;http://nbviewer.ipython.org/github/jeroenjanssens/jeroenjanssens.github.io/blob/master/Data%20Science%20at%20the%20Command%20Line%20-%20When%20is%20Fashion%20Week%20in%20New%20York%3F.ipynb&quot;&gt;view on nbviewer&lt;/a&gt;. Converting the book&amp;#39;s source code wouldn&amp;#39;t be too difficult, especially if we to convert it to Markdown and use &lt;a href=&quot;https://github.com/rossant/ipymd&quot;&gt;ipymd&lt;/a&gt;. What would be more challenging are packaging and distribution. &lt;/p&gt;

&lt;p&gt;The book introduces over 80 command-line tools, and installing them manually would take the better part of a day. I do offer a virtual machine based on Vagrant and VirtualBox that has everything installed, but I suspect there&amp;#39;s a better way to package this with IBash Notebook. For example, recently, &lt;a href=&quot;https://twitter.com/twiecki&quot;&gt;Thomas Wiecki&lt;/a&gt; created a Docker container that launches an IPython notebook server with the &lt;a href=&quot;https://registry.hub.docker.com/u/twiecki/pydata-docker-jupyterhub/&quot;&gt;PyData stack&lt;/a&gt; installed. And the &lt;a href=&quot;https://github.com/jupyter/tmpnb&quot;&gt;tmpnb&lt;/a&gt; project seems very promising as well. I must admit that I haven&amp;#39;t had time to look into Docker and these two projects at all. &lt;/p&gt;

&lt;p&gt;Distribution is then something I would need to figure out with my publisher O&amp;#39;Reilly Media. Considering the recent efforts for the book &lt;a href=&quot;https://github.com/ceteri/jem-docker&quot;&gt;Just Enough Math&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/odewahn&quot;&gt;Andew Odewahn&lt;/a&gt;, O&amp;#39;Reilly&amp;#39;s CTO, and their forward thinking regarding publishing in general, I foresee many opportunities.&lt;/p&gt;

&lt;h3&gt;What&amp;#39;s next?&lt;/h3&gt;

&lt;p&gt;Having inline Markdown, equations, and images sure is nice. However, in my opinion, the Bash kernel currently has two issues that hamper usability. First, the output is only printed when the command is finished; there are no real-time updates. This is especially inconvenient if you want to keep an eye on some long-running process using, say, &lt;code&gt;tail -f&lt;/code&gt; or &lt;code&gt;htop&lt;/code&gt;. Second, there&amp;#39;s no interactivity with the process possible. This means that you cannot drop into some other REPL like &lt;code&gt;julia&lt;/code&gt; or &lt;code&gt;psql&lt;/code&gt;. If there&amp;#39;s sufficient interest in IBash Notebook, then I suspect that these issues can be solved. Regardless, I believe that despite these two issues, IBash Notebook could very well serve as a means to introduce people to the command line.&lt;/p&gt;

&lt;p&gt;If you want to try out the Bash kernel for yourself, you should install &lt;a href=&quot;https://github.com/ipython/ipython&quot;&gt;IPython 3&lt;/a&gt; (which is currently in development). Then, you can clone the &lt;a href=&quot;https://github.com/takluyver/bash_kernel&quot;&gt;Bash kernel GitHub repository&lt;/a&gt; and install the package. (Best to do this all inside a virtual environment.) Next time you start a new notebook, you should be able to select the Bash kernel in the top-right corner.&lt;/p&gt;

&lt;p&gt;So, what do you think? Do you agree that IBash Notebook has potential? Am I crazy thinking that the command line can ever live outside the terminal? Would you like to see my book published as a collection of IBash notebooks? So many questions. Let me know on &lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Jeroen&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Rob Doherty and Adam Johnson for reading drafts of this.&lt;/em&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Feb 2015 11:00:00 +0100</pubDate>
				<link>http://www.jeroenjanssens.com/2015/02/19/ibash-notebook.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2015/02/19/ibash-notebook.html</guid>
			</item>
		
			<item>
				<title>Predicting at the command line @ PAPIs.io '14 [video]</title>
				<description>&lt;script src=&quot;http://www.youtube.com/player_api&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
var player = 0;
function onYouTubeIframeAPIReady() {
    player = new YT.Player('player');
}

function skipTo(time) {
    timeParts = time.innerHTML.split(&quot;:&quot;);
    minutes = parseInt(timeParts[0]);
    seconds = parseInt(timeParts[1]);
    player.seekTo((minutes * 60) + seconds);
}   
&lt;/script&gt;

&lt;p&gt;On November 17 of last year, I gave a tutorial at the first International Conference on Predictive APIs and Apps, also known as &lt;a href=&quot;http://www.papis.io/2014/&quot;&gt;PAPi&amp;#39;s.io &amp;#39;14&lt;/a&gt;. The recording is now available for your viewing pleasure:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;video&quot;&gt;&lt;/a&gt;
&lt;iframe id=&quot;player&quot; type=&quot;text/html&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;http://www.youtube.com/embed/3TieT2eOM1Y?enablejsapi=1&amp;autoplay=0&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;After a &lt;a href=&quot;http://lanyrd.com/2014/papis2014/schedule/&quot;&gt;myriad of API presentations&lt;/a&gt;, including those by &lt;a href=&quot;https://yhathq.com/&quot;&gt;Yhat&lt;/a&gt;, &lt;a href=&quot;https://bigml.com/&quot;&gt;BigML&lt;/a&gt;, and &lt;a href=&quot;https://dato.com/index.html&quot;&gt;GraphLab&lt;/a&gt;, 
I thought it would be nice to switch gears by demonstrating how such APIs can be employed from the command line (hence the title &amp;quot;Predicting at the Command Line&amp;quot;). &lt;/p&gt;

&lt;p&gt;Don&amp;#39;t worry about &lt;a href=&quot;http://lanyrd.com/2014/papis2014/sdfywx/&quot;&gt;the slides&lt;/a&gt;, they&amp;#39;re quite boring. More interesting is the live demo, in which I demonstrate how to use the command line to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;connect to the &lt;a href=&quot;https://www.bicing.cat/&quot;&gt;Barcelona bicing&lt;/a&gt; API using &lt;code&gt;curl&lt;/code&gt; (&lt;a href=&quot;#video&quot; onclick=&quot;skipTo(this); return false;&quot;&gt;12:30&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;convert the returned XML data to CSV using &lt;code&gt;xml2json&lt;/code&gt;, &lt;code&gt;jq&lt;/code&gt;, and &lt;code&gt;json2csv&lt;/code&gt; (&lt;a href=&quot;#video&quot; onclick=&quot;skipTo(this); return false;&quot;&gt;17:59&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;plot the locations of bike stations using &lt;code&gt;Rio&lt;/code&gt; and the &lt;a href=&quot;http://cran.r-project.org/web/packages/ggmap/index.html&quot;&gt;ggmap&lt;/a&gt; package (&lt;a href=&quot;#video&quot; onclick=&quot;skipTo(this); return false;&quot;&gt;27:18&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;assign anomaly scores using &lt;a href=&quot;&quot;&gt;BigML&lt;/a&gt;&amp;#39;s predictive API (&lt;a href=&quot;#video&quot; onclick=&quot;skipTo(this); return false;&quot;&gt;30:59&lt;/a&gt;), and as an additional impromptu demonstration, &lt;/li&gt;
&lt;li&gt;apply &lt;a href=&quot;http://www.indico.io&quot;&gt;indico&lt;/a&gt;&amp;#39;s language detection API to tweets using &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;parallel&lt;/code&gt; (&lt;a href=&quot;#video&quot; onclick=&quot;skipTo(this); return false;&quot;&gt;38:21&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In case you&amp;#39;d like to try this out for yourself, you should check out my book &lt;a href=&quot;http://datascienceatthecommandline.com/&quot;&gt;Data Science at the Command Line&lt;/a&gt; and the associated &lt;a href=&quot;http://datascienceatthecommandline.com/#dst&quot;&gt;Data Science Toolbox&lt;/a&gt;, which has all these command-line tools pre-installed.&lt;/p&gt;

&lt;p&gt;Cheers,&lt;/p&gt;

&lt;p&gt;Jeroen&lt;/p&gt;
</description>
				<pubDate>Mon, 16 Feb 2015 13:00:00 +0100</pubDate>
				<link>http://www.jeroenjanssens.com/2015/02/16/predicting-at-the-command-line-papis-2014-video.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2015/02/16/predicting-at-the-command-line-papis-2014-video.html</guid>
			</item>
		
			<item>
				<title>Data Science at the Command Line just got real!</title>
				<description>&lt;p&gt;I&amp;#39;m happy to tell you that my book Data Science at the Command Line just came out.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p&gt;Data Science at the Command Line just got real! &lt;a href=&quot;http://t.co/fgncBiJd9i&quot;&gt;http://t.co/fgncBiJd9i&lt;/a&gt; &lt;a href=&quot;http://t.co/1JjiDKIOc4&quot;&gt;pic.twitter.com/1JjiDKIOc4&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jeroen Janssens (@jeroenhjanssens) &lt;a href=&quot;https://twitter.com/jeroenhjanssens/status/520239869336248320&quot;&gt;October 9, 2014&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Its &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;companion website&lt;/a&gt; contains information about where to get it, past and upcoming related events, all the command-line tools it discusses, and the associated virtual machine. Now that the book is finished, I&amp;#39;m looking forward to write some more blog posts again.&lt;/p&gt;

&lt;p&gt;Best,&lt;/p&gt;

&lt;p&gt;Jeroen&lt;/p&gt;
</description>
				<pubDate>Thu, 09 Oct 2014 11:00:00 +0200</pubDate>
				<link>http://www.jeroenjanssens.com/2014/10/09/data-science-at-the-command-line-just-got-real.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2014/10/09/data-science-at-the-command-line-just-got-real.html</guid>
			</item>
		
			<item>
				<title>Lean, mean data science machine</title>
				<description>&lt;p&gt;&lt;em&gt;Update (1-4-2014) Be sure to check out the brand new &lt;a href=&quot;http://www.datasciencetoolbox.org&quot;&gt;Data Science Toolbox&lt;/a&gt;!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update (9-12-2013) I have compared my Vagrant environment with three other virtual environments for data science (&lt;a href=&quot;#comparison-of-virtual-environments-for-data-science&quot;&gt;see below&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data scientists love to create interesting models and exciting data visualizations. However, before they get to that point, usually much effort goes into obtaining, scrubbing, and exploring the required data. I argue that the *nix command-line, although invented decades ago, remains a powerful environment for processing data. It provides a read-eval-print loop (REPL) that is often much more convenient for exploratory data analysis than the edit-compile-run-debug cycle associated with large programs and even scripts.&lt;/p&gt;

&lt;p&gt;Unfortunately, setting up a workable environment and installing the latest command-line tools can be quite a pain. This post describes how to alleviate that pain and how to get you started doing data science on the command-line in a matter minutes.&lt;/p&gt;

&lt;h3&gt;Data Science at the Command Line&lt;/h3&gt;

&lt;p&gt;I am currently authoring a book titled &amp;quot;Data Science at the Command Line&amp;quot;, which will be published by O&amp;#39;Reilly in summer 2014.
The main goal of the book is to teach why, how, and when the command-line could be employed for data science. The tentative outline is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Getting Started&lt;/li&gt;
&lt;li&gt;Step 1: Obtaining Data&lt;/li&gt;
&lt;li&gt;Creating Reusable Command-line Tools&lt;/li&gt;
&lt;li&gt;Step 2: Scrubbing Data&lt;/li&gt;
&lt;li&gt;Managing Your Data Workflow&lt;/li&gt;
&lt;li&gt;Step 3: Exploring Data&lt;/li&gt;
&lt;li&gt;Speeding Up Data-Intensive Commands&lt;/li&gt;
&lt;li&gt;Step 4: Modeling Data&lt;/li&gt;
&lt;li&gt;Poor Man&amp;#39;s MapReduce&lt;/li&gt;
&lt;li&gt;Step 5: Interpreting Data&lt;/li&gt;
&lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Naturally, the book will be drenched with commands and source code. It is important that the text, the code, and the output of the code are consistent with each other. Manually running the code and copy-pasting the output is a cumbersome and error-prone process. 
To automate this process, I have created a script (a &lt;a href=&quot;http://www.dexy.it/&quot;&gt;dexy&lt;/a&gt; filter to be precise) that will (1) extract all the source code from the text, (2) run these in an isolated environment, and (3) paste the output back into the text. From here the O&amp;#39;Reilly toolchain takes over and converts the text to a variety of digital formats. Very smooth.&lt;/p&gt;

&lt;h3&gt;Your own Data Science Toolbox environment with Vagrant&lt;/h3&gt;

&lt;p&gt;The environment is created and configured using &lt;a href=&quot;http://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt;, which is basically a wrapper around VirtualBox and other virtualization software such AWS EC2. With a few commands, a fresh virtual machine is spun up and configured according to a simple script. It was &lt;a href=&quot;http://miningthesocialweb.com/2013/11/23/confessions-of-a-prolific-moonlighter-with-a-chronic-writing-disorder&quot;&gt;Matthew Russell&amp;#39;s Ignite talk&lt;/a&gt; that inspired me to use Vagrant; he provides one for his book &lt;a href=&quot;http://miningthesocialweb.com&quot;&gt;Mining the Social Web&lt;/a&gt; that is focused more on Python.
If my Vagrant environment would be provided with Data Science at the Command Line, then the reader would be able to follow along with the commands and source code. But since my mission is to enable everybody to do data science at the command-line as soon as possible, I have decided to make it available right now.&lt;/p&gt;

&lt;p&gt;Currently, the environment includes the &lt;a href=&quot;http://jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html&quot;&gt;seven command-line tools I discussed&lt;/a&gt; a while ago and &lt;a href=&quot;http://www.gnu.org/software/parallel/&quot;&gt;GNU parallel&lt;/a&gt;, which will be discussed in Chapter 8. Just like the book itself, the environment is a work in progress. In order to be able to run &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox/blob/master/tools/Rio&quot;&gt;Rio&lt;/a&gt; (one of the seven tools), I had to include the latest version of &lt;code&gt;R&lt;/code&gt;, together with the packages &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;sqldf&lt;/code&gt;, and &lt;code&gt;plyr&lt;/code&gt;. &lt;del&gt;I am aware that many of you would prefer the Python scientific stack to be included as well.&lt;/del&gt; The Python scientific stack (&lt;code&gt;ipython&lt;/code&gt;, &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, and &lt;code&gt;scikit-learn&lt;/code&gt;) is also included. However, because of disk-space and provision-time constraints, I doubt whether it is desirable (or even possible) to create an environment that includes everything. Perhaps that we can devise a solution where you select which tools, packages, and languages you would like to have installed. As mentioned, it is a work in progress and my main goal is to get you up and running on the command-line.&lt;/p&gt;

&lt;h3&gt;Installing the Data Science Toolbox environment&lt;/h3&gt;

&lt;p&gt;The environment is currently configured to run on top of &lt;a href=&quot;https://www.virtualbox.org&quot;&gt;VirtualBox&lt;/a&gt;. (I am looking into the option to deploy it on an AWS EC2 instance.)
So, first you will need to install &lt;a href=&quot;https://www.virtualbox.org&quot;&gt;VirtualBox&lt;/a&gt;. 
Second you need to install &lt;a href=&quot;http://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt;. 
Third, you need to download the environment by cloning the data science toolbox. (If you do not want to use &lt;code&gt;git&lt;/code&gt; you can also &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox/archive/master.zip&quot;&gt;download the zip file&lt;/a&gt;.)&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/jeroenjanssens/data-science-toolbox.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;data-science-toolbox/box
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Running &lt;code&gt;vagrant up&lt;/code&gt; in the &lt;code&gt;box&lt;/code&gt; directory will download the base box (Ubuntu 12.04 LTS 64-bit), spin up a virtual machine, and provision it. (Now would be the perfect time to think about any command-line scripts you may have lying around and donate them to the &lt;a href=&quot;http://datasciencetoolbox.org&quot;&gt;data science toolbox&lt;/a&gt;.) Once the provisioning is complete, you will be able to log into your own lean, mean data science machine: &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;vagrant ssh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Run the following command to test whether everything has been installed correctly:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -s &lt;span class=&quot;s1&quot;&gt;&amp;#39;http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; scrape -be &lt;span class=&quot;s1&quot;&gt;&amp;#39;table.wikitable &amp;gt; tr:not(:first-child)&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; xml2json &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; jq -c &lt;span class=&quot;s1&quot;&gt;&amp;#39;.html.body.tr[] | {country: .td[1][], border: .td[2][], surface: .td[3][], ratio: .td[4][]}&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; json2csv -p -k&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;country,ratio &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; Rio -se&lt;span class=&quot;s1&quot;&gt;&amp;#39;sqldf(&amp;quot;select * from df where ratio &amp;gt; 0.3 order by ratio desc&amp;quot;)&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; csvlook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;|----------------+------------|
|  country       | ratio      |
|----------------+------------|
|  Vatican City  | 7.2727273  |
|  Monaco        | 2.2        |
|  San Marino    | 0.6393443  |
|  Liechtenstein | 0.475      |
|----------------+------------|
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The virtual machine is not entirely isolated. Files that you put in the &lt;code&gt;box&lt;/code&gt; directory will be accessible from the &lt;code&gt;/vagrant&lt;/code&gt; directory in the virtual machine. This allows you to use both the tools you already have installed and the command-line tools provided by the environment. If you want to install any of these tools on your own machine, then you can run the relevant commands from the &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox/blob/master/box/bootstrap.sh&quot;&gt;provisioning script&lt;/a&gt;. &lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;comparison-of-virtual-environments-for-data-science&quot;&gt;&lt;/a&gt;Comparison of virtual environments for data science&lt;/h3&gt;

&lt;p&gt;Of course the Data Science Toolbox environment is not the only one available for doing data science! 
So far, I have been able to perform a rudimentary comparison with three other solutions.
(Please let me know if you know any others.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Data Science Toolbox (DST)&lt;/strong&gt; &lt;br /&gt;
Created by: &lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;Jeroen Janssens&lt;/a&gt; &lt;br /&gt;
Github: &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox&quot;&gt;jeroenjanssens/data-science-toolbox&lt;/a&gt; &lt;br /&gt;
Installs R, the Python scientific stack, and of course many command-line tools for processing data. Uses Vagrant and for now it can be deployed on VirtualBox, only.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Mining the Social Web (MTSW)&lt;/strong&gt; &lt;br /&gt;
Created by: &lt;a href=&quot;https://twitter.com/ptwobrussell&quot;&gt;Matthew Russel&lt;/a&gt; &lt;br /&gt;
Website: &lt;a href=&quot;http://miningthesocialweb.com/&quot;&gt;miningthesocialweb.com/&lt;/a&gt; &lt;br /&gt;
Github: &lt;a href=&quot;https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition&quot;&gt;ptwobrussell/Mining-the-Social-Web-2nd-Edition&lt;/a&gt; &lt;br /&gt;
Uses Vagrant (with Chef as the provisioner, which is really nice) and can be deployed on both VirtualBox and AWS.
Installs IPython Notebook, numpy, mongo, and NLTK, which allows you to follow along with the examples provided in the book. An AWS AMI is available as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Data Science Toolkit (DSTK)&lt;/strong&gt; &lt;br /&gt;
Created by: &lt;a href=&quot;https://twitter.com/petewarden&quot;&gt;Pete Warden&lt;/a&gt; &lt;br /&gt;
Website: &lt;a href=&quot;http://www.datasciencetoolkit.org&quot;&gt;www.datasciencetoolkit.org&lt;/a&gt; &lt;br /&gt;
Github: &lt;a href=&quot;https://github.com/petewarden/dstk&quot;&gt;petewarden/dstk&lt;/a&gt;  &lt;br /&gt;
The website provides a sandbox from which you can try out many interesting APIs. These APIs can also be accessed from the command line. An AWS AMI is available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Data Science Box (DSB)&lt;/strong&gt; &lt;br /&gt;
Created by: &lt;a href=&quot;https://twitter.com/drewconway&quot;&gt;Drew Conway&lt;/a&gt; &lt;br /&gt;
Github: &lt;a href=&quot;https://github.com/drewconway/data_science_box&quot;&gt;drewconway/data_science_box&lt;/a&gt; &lt;br /&gt;
This is a bash script for which you need have an AWS EC2 instance running.
It installs R, Shiny, IPython Notebook, and the Python scientific stack.&lt;/p&gt;

&lt;p&gt;For your convenience I have summarized this information in the following table.&lt;/p&gt;

&lt;table&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th /&gt;
            &lt;th&gt;Configuration&lt;/th&gt;
            &lt;th&gt;VirtualBox&lt;/th&gt;
            &lt;th&gt;AWS&lt;/th&gt;
            &lt;th&gt;AMI&lt;/th&gt;
            &lt;th&gt;Python&lt;/th&gt;
            &lt;th&gt;R&lt;/th&gt;
            &lt;th&gt;Shiny&lt;/th&gt;
            &lt;th&gt;Comments&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;th&gt;1. DST&lt;/th&gt;
            &lt;td&gt;Vagrant&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td&gt;Includes the Data Science Toolbox&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;2. MTSW&lt;/th&gt;
            &lt;td&gt;Vagrant&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;3. DSTK&lt;/th&gt;
            &lt;td&gt;Vagrant&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td&gt;Includes various command-line tools&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;th&gt;4. DSB&lt;/th&gt;
            &lt;td&gt;Bash&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;no&quot;&gt;No&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td class=&quot;yes&quot;&gt;Yes&lt;/td&gt;
            &lt;td&gt;&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In short, I think that they all have some strong aspects. Some of these may be improved over time (I am currently looking into using Chef as the provisioner), new environments may arise; that is the way open source works. In the end, it is up to you to decide which one works best for you. And if you want to make some tweaks, you can always fork the appropriate Github repository.&lt;/p&gt;

&lt;p&gt;It is in general just amazing to be able to spin up a new virtual machine with your own or somebody else&amp;#39;s environment, whether by running &lt;code&gt;vagrant up&lt;/code&gt; or by clicking a few buttons on AWS.&lt;/p&gt;

&lt;p&gt;I realize that three out of four names look really alike, which can be confusing, but it could also indicate that there is a need for having an automated (and isolated) setup to start doing data science without any additional hassle.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;While the command-line is a very powerful environment to process data, manually installing the latest command-line tools is not straightforward. Vagrant allows you to spin up a virtual machine and to install all the tools automatically.
In this post I have shared with you the exact same Vagrant environment as that I am using for my upcoming book, in the hope that it will be useful to get you started with doing data science at the command line. I have also compared my environment with three other virtual environments for data science.
Please let me know if you have any questions, suggestions, or contributions.&lt;/p&gt;

&lt;p&gt;Best wishes,&lt;/p&gt;

&lt;p&gt;Jeroen &lt;br&gt;
&lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;@jeroenhjanssens&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 07 Dec 2013 10:00:00 +0100</pubDate>
				<link>http://www.jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html</guid>
			</item>
		
			<item>
				<title>Stochastic Outlier Selection</title>
				<description>&lt;p&gt;&lt;em&gt;Update (13-12-2013) &lt;a href=&quot;http://www.hakkalabs.co/&quot;&gt;Hakka Labs&lt;/a&gt;, who kindly recorded my talk at the &lt;a href=&quot;http://www.meetup.com/NYC-Machine-Learning/events/149093182/&quot;&gt;NYC Machine Learning meetup&lt;/a&gt;, have put the &lt;a href=&quot;http://www.hakkalabs.co/articles/outlier-selection-and-one-class-classification-by-jeroen-janssens&quot;&gt;video and slides online&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My Ph.D., which I completed earlier this year, was about &lt;a href=&quot;https://github.com/jeroenjanssens/phd-thesis&quot;&gt;outlier selection and one-class classification&lt;/a&gt;. During this time I learned about quite a few machine learning algorithms; especially about outlier-selection algorithms and one-class classifiers, of course. With some help of &lt;a href=&quot;https://twitter.com/fhuszar&quot;&gt;Ferenc Huszár&lt;/a&gt; and &lt;a href=&quot;http://homepage.tudelft.nl/19j49/Home.html&quot;&gt;Laurens van der Maaten&lt;/a&gt;, I also came up with a new outlier-selection algorithm called &lt;a href=&quot;https://github.com/jeroenjanssens/sos&quot;&gt;Stochastic Outlier Selection&lt;/a&gt; (SOS), which I would like to briefly describe here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-densities.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;If you prefer a more detailed discussion about the algorithm, the experiments, and the results, you can read either the &lt;a href=&quot;https://github.com/jeroenjanssens/sos/blob/master/doc/sos-ticc-tr-2012-001.pdf?raw=true&quot;&gt;technical report (PDF)&lt;/a&gt; or chapter 4 of &lt;a href=&quot;https://github.com/jeroenjanssens/phd-thesis&quot;&gt;my Ph.D. thesis&lt;/a&gt;. In case you can&amp;#39;t wait to see whether your own dataset contains any outliers then there&amp;#39;s a &lt;a href=&quot;https://github.com/jeroenjanssens/sos&quot;&gt;Python implementation of SOS&lt;/a&gt; which you can also use from the command-line.&lt;/p&gt;

&lt;h3&gt;Affinity-based outlier selection&lt;/h3&gt;

&lt;p&gt;SOS is an unsupervised outlier-selection algorithm that takes as input either a feature matrix or a dissimilarity matrix and outputs for each data point an outlier probability. 
Intuitively, a data point is considered to be an outlier when the other data points have insufficient affinity with it. Allow me to explain this using the following two-dimensional toy dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-toydataset.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The right part of the figure shows that the feature matrix &lt;strong&gt;X&lt;/strong&gt; is transformed into a dissimilarity matrix &lt;strong&gt;D&lt;/strong&gt; using the Euclidean distance. (Any dissimilarity measure could have been used here.)
Using the dissimilarity matrix &lt;strong&gt;D&lt;/strong&gt;, SOS computes an affinity matrix &lt;strong&gt;A&lt;/strong&gt;, a binding probability matrix &lt;strong&gt;B&lt;/strong&gt;, and finally, the outlier probability vector &lt;strong&gt;Φ&lt;/strong&gt;, because Greek letters are cool.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-matrices.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The use of the concept of affinity is inspired by &lt;a href=&quot;http://homepage.tudelft.nl/19j49/t-SNE.html&quot;&gt;t-Distributed Stochastic Neighbor Embedding&lt;/a&gt; (t-SNE), which is a non-linear dimensionality reduction technique created by &lt;a href=&quot;http://homepage.tudelft.nl/19j49/Home.html&quot;&gt;Laurens van der Maaten&lt;/a&gt; and &lt;a href=&quot;http://www.cs.toronto.edu/%7Ehinton/&quot;&gt;Geoffrey Hinton&lt;/a&gt;. Both algorithms use the concept of affinity to quantify the relationship between data points. t-SNE uses it to preserve the local structure of a high-dimensional dataset and SOS uses it to select outliers.
The affinity a certain data point has with another data point decreases Gaussian-like with respect to their dissimilarity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-d2a.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Each data point has a variance associated with it. The variance depends on the density of the neighborhood. A higher density implies a lower variance. In fact, the variance is set such that each data point has effectively the same number of neighbors. 
This number is controlled via the only parameter of SOS, called perplexity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-variances.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;Perplexity can be interpreted as the &lt;em&gt;k&lt;/em&gt; in &lt;em&gt;k&lt;/em&gt;-nearest neighbor algorithms. The difference is that in SOS being a neighbor is not a binary property, but a probabilistic one. The following figure illustrates the binding probabilities data point &lt;strong&gt;x1&lt;/strong&gt; (or vertex &lt;strong&gt;v1&lt;/strong&gt; because we have switched to a graph representation of the dataset) has with the other five data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-binding.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The binding probability matrix is just the affinity matrix such that the rows sum to 1. To obtain the outlier probability of data point we compute the joint probability that the other data points will &lt;em&gt;not&lt;/em&gt; bind to it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-closedform.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;This simple equation corresponds to the intuition behind SOS mentioned earlier: a data point is considered to be an outlier when the other data points have insufficient affinity with it. The proof behind this equation is unfortunately beyond the scope of this post. &lt;/p&gt;

&lt;p&gt;SOS has been evaluated on a variety of real-world and synthetic datasets, and compared to four other outlier-selection algorithms. The following figure shows the weighted AUC performance on 18 real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/sos-results.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;As you can see, SOS has a higher performance on most of these real-world datasets. 
However, there&amp;#39;s still the no-free-lunch theorem, which basically says that no algorithm uniformly outperforms all other algorithms on all datasets. 
So, if you&amp;#39;d like to select some outliers on your own dataset, check out SOS by all means, but keep in mind that you may obtain a higher performance with a different outlier-selection algorithm. The real questions are: which one and why?&lt;/p&gt;

&lt;p&gt;As this was a very brief description of SOS, I had to skip over many details. 
Again, in case you&amp;#39;re interested, you can read either the &lt;a href=&quot;https://github.com/jeroenjanssens/sos/blob/master/doc/sos-ticc-tr-2012-001.pdf?raw=true&quot;&gt;technical report (PDF)&lt;/a&gt; or chapter 4 of &lt;a href=&quot;https://github.com/jeroenjanssens/phd-thesis&quot;&gt;my Ph.D. thesis&lt;/a&gt;. In the next section I apply SOS to roll call voting data. &lt;/p&gt;

&lt;h3&gt;&lt;a name=&quot;detecting-anomalous-senators&quot;&gt;&lt;/a&gt;Detecting anomalous senators&lt;/h3&gt;

&lt;p&gt;Last week, I had the pleasure to talk about outlier selection and one-class classification at the &lt;a href=&quot;http://www.meetup.com/NYC-Machine-Learning/events/149093182/&quot;&gt;NYC Machine Learning meetup&lt;/a&gt;. &lt;a href=&quot;http://www.hakkalabs.co/&quot;&gt;Hakka Labs&lt;/a&gt; recorded it, and put the &lt;a href=&quot;http://www.hakkalabs.co/articles/outlier-selection-and-one-class-classification-by-jeroen-janssens&quot;&gt;video and slides online&lt;/a&gt;. In order to not just show fancy graphs and boring equations I created a &lt;a href=&quot;http://bl.ocks.org/jeroenjanssens/7608890&quot;&gt;demo in D3 and CoffeeScript&lt;/a&gt;, of which you see a screenshot below. In the &lt;a href=&quot;http://bl.ocks.org/jeroenjanssens/7608890&quot;&gt;demo&lt;/a&gt;, I apply SOS on roll call voting data, which is inspired by &lt;a href=&quot;http://vikparuchuri.com/blog/how-divided-is-the-senate/&quot;&gt;this post on visualizing the senate&lt;/a&gt; by Vik Paruchuri. 
The demo illustrates how the approximated outlier probability of each senator evolves as more Stochastic Neighbor Graphs (SNG) are being sampled. (Please note that SNGs are not discussed in this post.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://bl.ocks.org/jeroenjanssens/7608890&quot;&gt;&lt;img src=&quot;/img/sos-senators.png&quot; alt=&quot;Detecting anomalous senators&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s see how the approximated outlier probabilities compare to the outlier probabilities computed on the command-line. Recently, I started using &lt;a href=&quot;https://github.com/Factual/drake#drake&quot;&gt;drake&lt;/a&gt; to organize my data workflow. (If you care about reproducibility, then I recommend you try it out.) The following &lt;code&gt;Drakefile&lt;/code&gt; shows how to fetch the roll call voting data, extract its features and labels, and apply the &lt;a href=&quot;https://github.com/jeroenjanssens/sos/blob/master/bin/sos&quot;&gt;Python implementation of SOS&lt;/a&gt; with a perplexity of 50 to it. &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;cat Drakefile

&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Get dataset&lt;/span&gt;
dataset.csv &amp;lt;- &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;-timecheck&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
    curl -s https://raw.github.com/VikParuchuri/political-positions/master/113_frame.csv &amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Extract features&lt;/span&gt;
features.csv &amp;lt;- dataset.csv
    csvcut &lt;span class=&quot;nv&quot;&gt;$INPUT&lt;/span&gt; -C 1,name,party,state &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; sed &lt;span class=&quot;s1&quot;&gt;&amp;#39;1d;s/NA/4/g&amp;#39;&lt;/span&gt; &amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Extract labels&lt;/span&gt;
labels.csv &amp;lt;- dataset.csv
    csvcut &lt;span class=&quot;nv&quot;&gt;$INPUT&lt;/span&gt; -c name,party,state &amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Compute outlier probabilities using SOS&lt;/span&gt;
outlier.csv &amp;lt;- features.csv
    &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;outlier&amp;#39;&lt;/span&gt; &amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;
    &amp;lt; &lt;span class=&quot;nv&quot;&gt;$INPUT&lt;/span&gt; sos -p &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# Combine labels and outlier probabilities and sort&lt;/span&gt;
result.csv &amp;lt;- labels.csv, outlier.csv
    paste -d, &lt;span class=&quot;nv&quot;&gt;$INPUT0&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$INPUT1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; csvsort -rc outlier &amp;gt; &lt;span class=&quot;nv&quot;&gt;$OUTPUT&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;drake
head result.csv | csvlook

|-------------+-------+-------+-------------|
|  name       | party | state | outlier     |
|-------------+-------+-------+-------------|
|  Cowan      | D     | MA    | 0.91758412  |
|  Lautenberg | D     | NJ    | 0.89442425  |
|  Chiesa     | R     | NJ    | 0.8457114   |
|  Markey     | D     | MA    | 0.7813504   |
|  Kerry      | D     | MA    | 0.75302407  |
|  Wyden      | D     | OR    | 0.70110306  |
|  Murkowski  | R     | AK    | 0.68868458  |
|  Alexander  | R     | TN    | 0.626972    |
|  Vitter     | R     | LA    | 0.59739462  |
|-------------+-------+-------+-------------|
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The tools &lt;code&gt;csvcut&lt;/code&gt;, &lt;code&gt;csvsort&lt;/code&gt;, and &lt;code&gt;csvlook&lt;/code&gt; are part of &lt;a href=&quot;http://csvkit.readthedocs.org/&quot;&gt;csvkit&lt;/a&gt;. 
You may notice that the outlier probabilities shown in the screenshot do not match the exact ones computed with &lt;code&gt;sos&lt;/code&gt;. That&amp;#39;s because (1) the screenshot was taken not long after the demo started and (2) the demo was running in Chrome, which apparently has a different implementation of &lt;code&gt;Math.random&lt;/code&gt;. In Firefox, the approximated outlier probabilities will match the exact ones, eventually.&lt;/p&gt;

&lt;p&gt;If you enjoyed this post then you may want to &lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;follow me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 24 Nov 2013 18:06:19 +0100</pubDate>
				<link>http://www.jeroenjanssens.com/2013/11/24/stochastic-outlier-selection.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/11/24/stochastic-outlier-selection.html</guid>
			</item>
		
			<item>
				<title>sudo make me a visualization! [video]</title>
				<description>&lt;p&gt;Last Monday I gave an Ignite talk at &lt;a href=&quot;http://strataconf.com/stratany2013/public/schedule/detail/32182&quot;&gt;Strata NYC&lt;/a&gt; about using command-line tools for data science.
Giving a talk where the slides auto-advance after 15 seconds is a challenging, but ultimately very rewarding experience! Even though the preview image below suggests otherwise, there&amp;#39;s no beatboxing in this video.&lt;/p&gt;

&lt;iframe style=&quot;display:block;margin: 0 auto;&quot; width=&quot;640&quot; height=&quot;360&quot; src=&quot;//www.youtube.com/embed/X__Z0phLxMY&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;On a related note, I&amp;#39;m writing a book titled &amp;quot;Data Science at the Command-line&amp;quot;, which will be published by O&amp;#39;Reilly somewhere in 2014. If you&amp;#39;re interested in receiving updates regarding the book then you should &lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;follow me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Thu, 31 Oct 2013 21:00:00 +0100</pubDate>
				<link>http://www.jeroenjanssens.com/2013/10/31/sudo-make-me-a-visualization.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/10/31/sudo-make-me-a-visualization.html</guid>
			</item>
		
			<item>
				<title>7 command-line tools for data science</title>
				<description>&lt;p&gt;&lt;em&gt;Newsflash! On October 5, I&amp;#39;m giving a one-day, hands-on &lt;a href=&quot;http://datascienceatthecommandline.com/#workshop&quot;&gt;workshop&lt;/a&gt; in Budapest.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://datascienceatthecommandline.com/#workshop&quot;&gt;&lt;img src=&quot;/img/workshop-crunching-data-at-the-command-line.png&quot; style=&quot;height:314px; width:600px;&quot;alt=&quot;Workshop Crunching Data at the Command Line&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update (23-5-2015) I&amp;#39;m now also doing &lt;a href=&quot;http://jeroenjanssens.com/consulting-and-training&quot;&gt;consulting and training&lt;/a&gt; on this exciting topic.&lt;/em&gt; &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update (7-17-2014) Check out my new book &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;Data Science at the Command Line&lt;/a&gt;, which contains over 70 command-line tools for doing data science.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data science is &lt;a href=&quot;http://www.dataists.com/2010/09/a-taxonomy-of-data-science/&quot;&gt;OSEMN&lt;/a&gt; (pronounced as awesome).
That is, it involves Obtaining, Scrubbing, Exploring, Modeling, and iNterpreting data.
As a data scientist, I spend quite a bit of time on the command-line, especially when there&amp;#39;s data to be obtained, scrubbed, or explored. And I&amp;#39;m not alone in this.
Recently, &lt;a href=&quot;http://www.gregreda.com/2013/07/15/unix-commands-for-data-science/&quot;&gt;Greg Reda discussed&lt;/a&gt; how the classics (e.g., head, cut, grep, sed, and awk) can be used for data science. Prior to that, Seth Brown discussed how to perform basic &lt;a href=&quot;http://www.drbunsen.org/explorations-in-unix/&quot;&gt;exploratory data analysis in Unix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I would like to continue this discussion by sharing seven command-line tools that I have found useful in my day-to-day work. 
The tools are:
&lt;a href=&quot;http://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jehiah/json2csv&quot;&gt;json2csv&lt;/a&gt;,
&lt;a href=&quot;https://github.com/onyxfish/csvkit&quot;&gt;csvkit&lt;/a&gt;,
scrape,
&lt;a href=&quot;https://github.com/parmentf/xml2json&quot;&gt;xml2json&lt;/a&gt;,
sample, and Rio. (The home-made tools &lt;code&gt;scrape&lt;/code&gt;, &lt;code&gt;sample&lt;/code&gt;, and &lt;code&gt;Rio&lt;/code&gt; can be found in this &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox&quot;&gt;data science toolbox&lt;/a&gt;.) Any suggestions, questions, comments, and even pull requests are more than welcome.
(Tools suggested by others can be found towards the bottom of the post.)
OSEMN, let&amp;#39;s get started with our first tool: &lt;code&gt;jq&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;1. jq - sed for JSON&lt;/h3&gt;

&lt;p&gt;JSON is becoming an increasingly common data format, especially as APIs are appearing everywhere. I remember cooking up the ugliest &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;sed&lt;/code&gt; incantations in order to process JSON. Thanks to &lt;code&gt;jq&lt;/code&gt;, those days are now in the past. &lt;/p&gt;

&lt;p&gt;Imagine we&amp;#39;re interested in the candidate totals of the 2008 presidential election. It so happens that the New York Times has a &lt;a href=&quot;http://developer.nytimes.com/docs/campaign_finance_api/&quot;&gt;Campaign Finance API&lt;/a&gt;. (You can &lt;a href=&quot;http://developer.nytimes.com/apps/mykeys&quot;&gt;get your own API keys&lt;/a&gt; if you want to access any of their APIs.) Let&amp;#39;s get some JSON using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -s &lt;span class=&quot;s1&quot;&gt;&amp;#39;http://api.nytimes.com/svc/elections/us/v3/finances/2008/president/totals.json?api-key=super-secret&amp;#39;&lt;/span&gt; &amp;gt; nyt.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;where &lt;code&gt;-s&lt;/code&gt; puts &lt;code&gt;curl&lt;/code&gt; in silent mode. In its simplest form, i.e., &lt;code&gt;jq &amp;#39;.&amp;#39;&lt;/code&gt;, the tool transforms the incomprehensible API response we got:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;{&amp;quot;status&amp;quot;:&amp;quot;OK&amp;quot;,&amp;quot;base_uri&amp;quot;:&amp;quot;http://api.nytimes.com/svc/elections/us/v3/finances/2008/&amp;quot;,&amp;quot;cycle&amp;quot;:2008,&amp;quot;copyright&amp;quot;:&amp;quot;Copyright (c) 2013 The New York Times Company. All Rights Reserved.&amp;quot;,&amp;quot;results&amp;quot;:[{&amp;quot;candidate_name&amp;quot;:&amp;quot;Obama, Barack&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Barack Obama&amp;quot;,&amp;quot;party&amp;quot;:&amp;quot;D&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;into nicely indented and colored output:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; nyt.json jq &lt;span class=&quot;s1&quot;&gt;&amp;#39;.&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; head
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;quot;results&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;candidate_id&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;P80003338&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;date_coverage_from&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;2007-01-01&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;date_coverage_to&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;2008-11-24&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;candidate_name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Obama, Barack&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Barack Obama&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;quot;party&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the output isn&amp;#39;t necessarily in the same order as the input.
Besides pretty printing, &lt;code&gt;jq&lt;/code&gt; can also select, filter, and format JSON data, as illustrated by 
the following command, which returns the name, cash, and party of each candidate that had at least $1,000,000 in cash:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; nyt.json jq -c &lt;span class=&quot;s1&quot;&gt;&amp;#39;.results[] | {name, party, cash: .cash_on_hand} | select(.cash | tonumber &amp;gt; 1000000)&amp;#39;&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;cash&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;29911984.0&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;party&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Barack Obama&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;cash&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;32812513.75&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;party&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;R&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;John McCain&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;cash&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;4428347.5&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;party&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;D&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;John Edwards&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;http://stedolan.github.io/jq/manual/&quot;&gt;jq manual&lt;/a&gt; to read about the many other things it can do, but don&amp;#39;t expect it to solve all your data munging problems. 
Remember, the Unix philosophy favors small programs that do one thing and do it well. 
And &lt;code&gt;jq&lt;/code&gt;&amp;#39;s functionality is more than sufficient I would say! 
Now that we have the data we need, it&amp;#39;s time to move on to our second tool: &lt;code&gt;json2csv&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;2. json2csv - convert JSON to CSV&lt;/h3&gt;

&lt;p&gt;While JSON is a great format for interchanging data, it&amp;#39;s rather unsuitable for most command-line tools. Not to worry, we can easily convert JSON into CSV using &lt;a href=&quot;https://github.com/jehiah/json2csv&quot;&gt;json2csv&lt;/a&gt;. Assuming that we stored the data from the last step in &lt;code&gt;million.json&lt;/code&gt;, simply invoking&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; million.json json2csv -k name,party,cash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;will convert it to some nicely comma-separated values:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-awk&quot; data-lang=&quot;awk&quot;&gt;&lt;span class=&quot;nx&quot;&gt;Barack&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Obama&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;29911984.0&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;John&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;McCain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;32812513.75&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;John&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Edwards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4428347.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Having the data in CSV format allows us to use the classic tools such as &lt;code&gt;cut -d,&lt;/code&gt; and &lt;code&gt;awk -F,&lt;/code&gt;. 
Others like &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;sed&lt;/code&gt; don&amp;#39;t really have a notion of fields. 
Since CSV is the king of tabular file formats, according to the authors of 
&lt;a href=&quot;http://csvkit.readthedocs.org/&quot;&gt;csvkit&lt;/a&gt;, they created, well, &lt;code&gt;csvkit&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;3. csvkit - suite of utilities for converting to and working with CSV&lt;/h3&gt;

&lt;p&gt;Rather than being one tool, &lt;a href=&quot;http://csvkit.readthedocs.org/&quot;&gt;csvkit&lt;/a&gt; is a collection of tools that operate on CSV data. Most of these tools expect the CSV data to have a header, so let&amp;#39;s add one. (Since the publication of this post, &lt;code&gt;json2csv&lt;/code&gt; has been updated to print the header with the &lt;code&gt;-p&lt;/code&gt; option.)&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;name,party,cash &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; cat - million.csv &amp;gt; million-header.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We can, for example, sort the candidates by cash with &lt;code&gt;csvsort&lt;/code&gt; and display the data using &lt;code&gt;csvlook&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; million-header.csv csvsort -rc cash &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; csvlook

&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;---------------+-------+--------------&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;  name         &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; party &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; cash         &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;---------------+-------+--------------&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;  John McCain  &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; R     &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; 32812513.75  &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;  Barack Obama &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; D     &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; 29911984.0   &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;  John Edwards &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; D     &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; 4428347.5    &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;---------------+-------+--------------&lt;span class=&quot;p&quot;&gt;|&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Looks like the MySQL console doesn&amp;#39;t it? Speaking of databases, you can insert the CSV data into an sqlite database as follows (many other databases are supported as well):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;csvsql --db sqlite:///myfirst.db --insert million-header.csv
sqlite3 myfirst.db
sqlite&amp;gt; .schema million-header
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TABLE&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;&amp;quot;million-header&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;party&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;VARCHAR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;cash&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;FLOAT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this case, the database columns have the correct data types because the type is inferred from the CSV data.
 Other tools within &lt;code&gt;csvkit&lt;/code&gt; that might be of interest are: &lt;code&gt;in2csv&lt;/code&gt;, &lt;code&gt;csvgrep&lt;/code&gt;, and &lt;code&gt;csvjoin&lt;/code&gt;. And with &lt;code&gt;csvjson&lt;/code&gt;, the data can even be converted back to JSON. All in all, &lt;code&gt;csvkit&lt;/code&gt; is worth &lt;a href=&quot;http://csvkit.readthedocs.org/&quot;&gt;checking out&lt;/a&gt;. &lt;/p&gt;

&lt;h3&gt;4. scrape - HTML extraction using XPath or CSS selectors&lt;/h3&gt;

&lt;p&gt;JSON APIs sure are nice, but they aren&amp;#39;t the only source of data; a lot of it is &lt;a href=&quot;http://www.ted.com/talks/tim_berners_lee_on_the_next_web.html&quot;&gt;unfortunately still&lt;/a&gt; embedded in HTML. &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox&quot;&gt;scrape&lt;/a&gt; is a python script I put together that employs the &lt;code&gt;lxml&lt;/code&gt; and &lt;code&gt;cssselect&lt;/code&gt; packages to select certain HTML elements by means of an XPath query or &lt;a href=&quot;http://net.tutsplus.com/tutorials/html-css-techniques/the-30-css-selectors-you-must-memorize/&quot;&gt;CSS selector&lt;/a&gt;. (I tried &lt;a href=&quot;https://metacpan.org/module/scrape.pl&quot;&gt;scrape.pl&lt;/a&gt;, but I couldn&amp;#39;t get it to work properly. Moreover, rather than processing HTML from stdin, it expects a url and then downloads the HTML itself.)
Let&amp;#39;s extract the table from &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio&quot;&gt;this Wikipedia article that lists the border and area ratio of each country&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -s &lt;span class=&quot;s1&quot;&gt;&amp;#39;http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; scrape -b -e &lt;span class=&quot;s1&quot;&gt;&amp;#39;table.wikitable &amp;gt; tr:not(:first-child)&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; head
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;tr&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt;1&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt;Vatican City&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt;3.2&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt;0.44&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt;7.2727273&lt;span class=&quot;nt&quot;&gt;&amp;lt;/td&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/tr&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;-b&lt;/code&gt; argument lets &lt;code&gt;scrape&lt;/code&gt; enclose the output with &lt;code&gt;&amp;lt;html&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;body&amp;gt;&lt;/code&gt; tags, which is sometimes required by &lt;code&gt;xml2json&lt;/code&gt; to convert correctly the HTML to JSON.&lt;/p&gt;

&lt;h3&gt;5. xml2json - convert XML to JSON&lt;/h3&gt;

&lt;p&gt;As its name implies, &lt;a href=&quot;https://github.com/parmentf/xml2json&quot;&gt;xml2json&lt;/a&gt; takes XML (and HTML) as input and returns JSON as output. Therefore, &lt;code&gt;xml2json&lt;/code&gt; is a great liaison between &lt;code&gt;scrape&lt;/code&gt; and &lt;code&gt;jq&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -s &lt;span class=&quot;s1&quot;&gt;&amp;#39;http://en.wikipedia.org/wiki/List_of_countries_and_territories_by_border/area_ratio&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; scrape -be &lt;span class=&quot;s1&quot;&gt;&amp;#39;table.wikitable &amp;gt; tr:not(:first-child)&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; xml2json &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; jq -c &lt;span class=&quot;s1&quot;&gt;&amp;#39;.html.body.tr[] | {country: .td[1][], border: .td[2][], surface: .td[3][], ratio: .td[4][]}&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; head
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;7.2727273&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.44&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;3.2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Vatican City&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;2.2000000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;4.4&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Monaco&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.6393443&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;61&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;39&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;San Marino&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.4750000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;160&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;76&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Liechtenstein&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.3000000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;34&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;10.2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Sint Maarten (Netherlands)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.2570513&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;468&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;120.3&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Andorra&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.2000000&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;6&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;1.2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Gibraltar (United Kingdom)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.1888889&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;54&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;10.2&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Saint Martin (France)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.1388244&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;2586&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;359&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Luxembourg&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;ratio&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;0.0749196&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;surface&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;6220&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;border&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;466&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;quot;country&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Palestinian territories&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Of course this JSON data could then be piped into &lt;code&gt;json2csv&lt;/code&gt; and so forth.&lt;/p&gt;

&lt;h3&gt;6. sample - when you&amp;#39;re in debug mode&lt;/h3&gt;

&lt;p&gt;The second tool I made is &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox/blob/master/tools/sample&quot;&gt;sample&lt;/a&gt;. (It&amp;#39;s based on two scripts in &lt;a href=&quot;https://github.com/bitly/data_hacks&quot;&gt;bitly&amp;#39;s data_hacks&lt;/a&gt;, which contains some other tools worth checking out.) When you&amp;#39;re in the process of formulating your data pipeline and you have a lot of data, then debugging your pipeline can be cumbersome. In that case, &lt;code&gt;sample&lt;/code&gt; might be useful. 
The tool serves three purposes (which isn&amp;#39;t very Unix-minded, but since it&amp;#39;s mostly useful when you&amp;#39;re in debug mode, that&amp;#39;s not such a big deal). &lt;/p&gt;

&lt;p&gt;The first purpose of &lt;code&gt;sample&lt;/code&gt; is to get a subset of the data by outputting only a certain percentage of the input on a line-by-line basis. The second purpose is to add some delay to the output. This comes in handy when the input is a constant stream (e.g., the Twitter firehose), and the data comes in too fast to see what&amp;#39;s going on.
The third purpose is to run only for a certain time. The following invocation illustrates all three purposes.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;seq &lt;span class=&quot;m&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; sample -r 20% -d &lt;span class=&quot;m&quot;&gt;1000&lt;/span&gt; -s &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; jq &lt;span class=&quot;s1&quot;&gt;&amp;#39;{number: .}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This way, every input line has a 20% chance of being forwarded to &lt;code&gt;jq&lt;/code&gt;. Moreover, there is a 1000 millisecond delay between each line and after five seconds &lt;code&gt;sample&lt;/code&gt; will stop entirely. Please note that each argument is optional. 
In order to prevent unnecessary computation, try to put &lt;code&gt;sample&lt;/code&gt; as early as possible in your pipeline (the same argument holds for &lt;code&gt;head&lt;/code&gt; and &lt;code&gt;tail&lt;/code&gt;). Once you&amp;#39;re done debugging you can simply take it out of the pipeline.&lt;/p&gt;

&lt;h3&gt;7. Rio - making R part of the pipeline&lt;/h3&gt;

&lt;p&gt;This post wouldn&amp;#39;t be complete without some R.
It&amp;#39;s not straightforward to make R/Rscript part of the pipeline since they don&amp;#39;t 
work with stdin and stdout out of the box.
Therefore, as a proof of concept, I put together a bash script called &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox/blob/master/tools/Rio&quot;&gt;Rio&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;&lt;code&gt;Rio&lt;/code&gt; works as follows.
First, the CSV provided to stdin is redirected to a temporary file and lets R read that into a data frame &lt;code&gt;df&lt;/code&gt;.
Second, the specified commands in the &lt;code&gt;-e&lt;/code&gt; option are executed.
Third, the output of the last command is redirected to stdout. 
Allow me to demonstrate three one-liners that use the Iris dataset (don&amp;#39;t mind the url).&lt;/p&gt;

&lt;p&gt;Display the five-number-summary of each field.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -s &lt;span class=&quot;s1&quot;&gt;&amp;#39;https://raw.github.com/pydata/pandas/master/pandas/tests/data/iris.csv&amp;#39;&lt;/span&gt; &amp;gt; iris.csv
&amp;lt; iris.csv Rio -e &lt;span class=&quot;s1&quot;&gt;&amp;#39;summary(df)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;  SepalLength      SepalWidth     PetalLength      PetalWidth   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.054   Mean   :3.759   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
     Name          
 Length:150        
 Class :character  
 Mode  :character 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you specify the &lt;code&gt;-s&lt;/code&gt; option, the &lt;code&gt;sqldf&lt;/code&gt; package will be imported.
In case tthe output is a data frame, CSV will be written to stdout. This enables you to further process that data using other tools.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; iris.csv Rio -se &lt;span class=&quot;s1&quot;&gt;&amp;#39;sqldf(&amp;quot;select * from df where df.SepalLength &amp;gt; 7.5&amp;quot;)&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; csvlook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;|--------------+------------+-------------+------------+-----------------|
|  SepalLength | SepalWidth | PetalLength | PetalWidth | Name            |
|--------------+------------+-------------+------------+-----------------|
|  7.6         | 3          | 6.6         | 2.1        | Iris-virginica  |
|  7.7         | 3.8        | 6.7         | 2.2        | Iris-virginica  |
|  7.7         | 2.6        | 6.9         | 2.3        | Iris-virginica  |
|  7.7         | 2.8        | 6.7         | 2          | Iris-virginica  |
|  7.9         | 3.8        | 6.4         | 2          | Iris-virginica  |
|  7.7         | 3          | 6.1         | 2.3        | Iris-virginica  |
|--------------+------------+-------------+------------+-----------------|
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you specify the &lt;code&gt;-g&lt;/code&gt; option, &lt;code&gt;ggplot2&lt;/code&gt; gets imported and a ggplot object called &lt;code&gt;g&lt;/code&gt; with &lt;code&gt;df&lt;/code&gt; as the data is initialized.
If the final output is a ggplot object, a PNG will be written to stdout.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;lt; iris.csv Rio -ge &lt;span class=&quot;s1&quot;&gt;&amp;#39;g+geom_point(aes(x=SepalLength,y=SepalWidth,colour=Name))&amp;#39;&lt;/span&gt; &amp;gt; iris.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/img/iris.png&quot; alt=&quot;iris.csv&quot;&gt;&lt;/p&gt;

&lt;p&gt;I made this tool so that I could take advantage of the power of R on the command-line. Of course it has its limits, but at least there&amp;#39;s no need to learn &lt;a href=&quot;http://www.gnuplot.info&quot;&gt;gnuplot&lt;/a&gt; any more.&lt;/p&gt;

&lt;h3&gt;Command-line tools suggested by others&lt;/h3&gt;

&lt;p&gt;Below is an uncurated list of tools and repositories that others have suggested via &lt;a href=&quot;https://twitter.com/jeroenhjanssens/&quot;&gt;twitter&lt;/a&gt; or &lt;a href=&quot;https://news.ycombinator.com/item?id=6412190&quot;&gt;Hacker News&lt;/a&gt; (last updated on 23-09-2013 07:15 EST). Thanks everybody.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://bigmler.readthedocs.org/en/latest/&quot;&gt;BigMLer&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=aficionado&quot;&gt;aficionado&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://code.google.com/p/crush-tools/&quot;&gt;crush-tools&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=mjn&quot;&gt;mjn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dergachev/csv2sqlite&quot;&gt;csv2sqlite&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=dergachev&quot;&gt;dergachev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dbro/csvquote&quot;&gt;csvquote&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=susi22&quot;&gt;susi22&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/clarkgrubb/data-tools&quot;&gt;data-tools repository&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=cgrubb&quot;&gt;cgrubb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dkogan/feedgnuplot&quot;&gt;feedgnuplot&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=dima55&quot;&gt;dima55&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cgutteridge/Grinder/tree/master/bin&quot;&gt;Grinder repository&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/cgutteridge&quot;&gt;@cgutteridge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.hdfgroup.org/HDF5/doc/RM/Tools.html&quot;&gt;HDF5 Tools&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=susi22&quot;&gt;susi22&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://code.google.com/p/littler/&quot;&gt;littler&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/eddelbuettel&quot;&gt;@eddelbuettel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://gibrown.wordpress.com/2013/01/26/unix-bi-grams-tri-grams-and-topic-modeling/&quot;&gt;mallet&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=gibrown&quot;&gt;gibrown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/benbernard/RecordStream&quot;&gt;RecordStream&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=revertts&quot;&gt;revertts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/paulgb/subsample&quot;&gt;subsample&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=paulgb&quot;&gt;paulgb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://search.cpan.org/%7Eken/xls2csv-1.07/script/xls2csv&quot;&gt;xls2csv&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/sheeshee&quot;&gt;@sheeshee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://xmlstar.sourceforge.net/&quot;&gt;XMLStarlet&lt;/a&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=gav&quot;&gt;gav&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I have shown you seven command-line tools that I use in my daily work as a data scientist.
While each tool is useful in its own way, I often find myself combining them with, or just resorting to, the classics such as &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;sed&lt;/code&gt;, and &lt;code&gt;awk&lt;/code&gt;. Combining such small tools into a larger pipeline is what makes them really powerful.&lt;/p&gt;

&lt;p&gt;I&amp;#39;m curious to hear what you think about this list and what command-line tools you like to use.
Also, if you&amp;#39;ve made any tools yourself, you&amp;#39;re more than welcome to add them to this &lt;a href=&quot;https://github.com/jeroenjanssens/data-science-toolbox&quot;&gt;data science toolbox&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Don&amp;#39;t worry if you don&amp;#39;t regard yourself as a toolmaker. The next time you&amp;#39;re cooking up that exotic pipeline, consider to put it in a file, add a &lt;a href=&quot;http://en.wikipedia.org/wiki/Shebang_%28Unix%29&quot;&gt;shebang&lt;/a&gt;, parametrize it with some &lt;code&gt;$1&lt;/code&gt;s and &lt;code&gt;$2&lt;/code&gt;s, and &lt;code&gt;chmod +x&lt;/code&gt; it. That&amp;#39;s all there is to it. Who knows, you might even become interested in applying the &lt;a href=&quot;http://www.faqs.org/docs/artu/ch01s06.html&quot;&gt;Unix philosophy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While the power of the command-line should not be underestimated when it comes to Obtaining, Scrubbing, and Exploring data, it can only get you so far. When you&amp;#39;re ready to do some more serious Exploring, Modelling, and iNterpretation of your data, you&amp;#39;re probably better off continuing your work in a statistical computing environment, such as &lt;a href=&quot;http://www.r-project.org/&quot;&gt;R&lt;/a&gt; or &lt;a href=&quot;http://ipython.org/notebook.html&quot;&gt;IPython notebook&lt;/a&gt;+&lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you enjoyed this post, then you may be interested in my new book &lt;a href=&quot;http://datascienceatthecommandline.com&quot;&gt;Data Science at the Command Line&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Sep 2013 14:00:00 +0200</pubDate>
				<link>http://www.jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html</guid>
			</item>
		
			<item>
				<title>Extracting text from HTML with Reporter</title>
				<description>&lt;p&gt;&lt;em&gt;Note: This post originally appeared on November 11, 2012 on &lt;a href=&quot;http://visualrevenue.com/blog&quot;&gt;Visual Revenue&amp;#39;s blog&lt;/a&gt;. The text from the &lt;a href=&quot;https://github.com/visualrevenue/reporter&quot;&gt;Github repository&lt;/a&gt; has been appended for completeness.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At Visual Revenue, we use many great open source software packages. Our favorites include: &lt;a href=&quot;http://www.ubuntu.com/&quot;&gt;ubuntu&lt;/a&gt;, &lt;a href=&quot;http://www.vim.org/&quot;&gt;vim&lt;/a&gt;, &lt;a href=&quot;http://www.gnu.org/software/emacs/&quot;&gt;emacs&lt;/a&gt;, &lt;a href=&quot;http://git-scm.com/&quot;&gt;git&lt;/a&gt;, &lt;a href=&quot;http://www.mongodb.org/&quot;&gt;mongodb&lt;/a&gt;, &lt;a href=&quot;http://seleniumhq.org/&quot;&gt;selenium&lt;/a&gt;, &lt;a href=&quot;http://redis.io/&quot;&gt;redis&lt;/a&gt;, &lt;a href=&quot;http://ipython.org/&quot;&gt;ipython&lt;/a&gt;, &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;scikit-learn&lt;/a&gt;, &lt;a href=&quot;http://pandas.pydata.org/&quot;&gt;pandas&lt;/a&gt;, &lt;a href=&quot;http://www.r-project.org/&quot;&gt;R&lt;/a&gt;, and &lt;a href=&quot;http://d3js.org/&quot;&gt;D3&lt;/a&gt;. This open source tech helps us to develop a state-of-the-art recommendation platform for news editors. I&amp;#39;m happy to announce that we can now give something back to the open source community: Reporter.&lt;/p&gt;

&lt;p&gt;Reporter is being developed at &lt;a href=&quot;http://www.visualrevenue.com&quot;&gt;Visual Revenue, Inc.&lt;/a&gt; where it is used to extract the main text from news articles. The name Reporter and internal terms are inspired by the news domain.&lt;/p&gt;

&lt;p&gt;Reporter is a flexible tool that extracts text from HTML. At Visual Revenue, we actively use it to extract the main text from news articles. It is written in Python, which allows you to embed it in your own Python applications. You can also use Reporter from the command line.&lt;/p&gt;

&lt;p&gt;In short, Reporter:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;  Extracts the main text from HTML.&lt;/li&gt;
&lt;li&gt;  Uses a white-box scoring algorithm to determine the main text container.&lt;/li&gt;
&lt;li&gt;  Can easily be extended.&lt;/li&gt;
&lt;li&gt;  Supports Unicode without pain.&lt;/li&gt;
&lt;li&gt;  Has awesome debugging facilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below, I&amp;#39;ll explain how the scoring algorithm works.&lt;/p&gt;

&lt;h3&gt;Download&lt;/h3&gt;

&lt;p&gt;You can read more about Reporter on Github:
&lt;a href=&quot;https://github.com/visualrevenue/reporter&quot;&gt;https://github.com/visualrevenue/reporter&lt;/a&gt;. It&amp;#39;s also available as a PyPi package:
&lt;a href=&quot;http://pypi.python.org/pypi/reporter/0.1.2&quot;&gt;http://pypi.python.org/pypi/reporter/0.1.2&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Usage&lt;/h3&gt;

&lt;p&gt;Reporter can be invoked from the command line:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;reporter.py --url URL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The HTML from URL will be parsed by &lt;a href=&quot;http://www.crummy.com/software/BeautifulSoup/bs4/doc/&quot;&gt;Beautiful Soup&lt;/a&gt; and the main
text will be printed on stdout. If the &lt;strong&gt;--debug&lt;/strong&gt; flag is added, the text and HTML will be saved to file. The HTML will be styled as follows. Each tag will get a background color based on its score, ranging from red (low score) to green (high score). Moreover, the tag that is selected as news container (see below) will have a blue dashed line.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/scoring.png&quot; alt=&quot;Scoring&quot;&gt;&lt;/p&gt;

&lt;p&gt;If the &lt;strong&gt;--test&lt;/strong&gt; flag is given, all files in ./test/input will be processed, and the text and HTML will be saved, as in &lt;strong&gt;--debug&lt;/strong&gt;. This is useful for processing many local files, so that these only have to downloaded once. &lt;/p&gt;

&lt;p&gt;Please see &lt;strong&gt;./reporter.py --help&lt;/strong&gt; for more options.&lt;/p&gt;

&lt;p&gt;Reporter can also be used from Python:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;my_reporter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Reporter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_reporter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;http://example.com&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_reporter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;report_news&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Scoring algorithm&lt;/h3&gt;

&lt;p&gt;To extract the main text from an HTML document, Reporter gives each HTML tag (e.g., DIV, H1, and P) a score. The text contained in the tag with the highest score is returned as the main text of the news article.&lt;/p&gt;

&lt;p&gt;The main part of the scoring algorithm is based on traversing the parsed HTML and works as follows. Reporter traverses the HTML in reverse order, i.e., it starts at the leaves of the DOM tree. Each tag is scored either as a paragraph or as a container. A tag is considered to be a paragraph (in the abstract sense, not in the P sense) when it contains more than 10 characters*, otherwise it is considered to be a container. The exact scoring of a tag is defined in the &lt;strong&gt;Autocue&lt;/strong&gt;. An Autocue is a list of scoring rules that get triggered at various stages. For example, when a tag is to be scored as a paragraph, one rule may count the number of words and return 2 points per word. Once a tag (and its siblings) are scored, its parent is scored. If the parent is also considered to be a paragraph, which happens, for
instance, with the P tag in: &lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;DIV&amp;gt;&amp;lt;P&amp;gt;&lt;/span&gt;Hello World, this is the &lt;span class=&quot;nt&quot;&gt;&amp;lt;B&amp;gt;&lt;/span&gt;Reporter package&lt;span class=&quot;nt&quot;&gt;&amp;lt;/B&amp;gt;&amp;lt;/P&amp;gt;&amp;lt;/DIV&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;the scores of the B tag are discarded and the complete text is re-scored. The DIV tag is scored as a container because (in this case) it contains no text by itself. In fact, there is
an important scoring rule which penalises containers. If such a rule would not be included, the HTML tag would always receive the highest score, which would not be very effective. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;*) Currently, this is the only heuristic that is hard-coded. In
&lt;a href=&quot;https://github.com/gfxmonk/python-readability&quot;&gt;Readability&lt;/a&gt;, which served as the inspiration for Reporter, all scoring is hard-coded.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As mentioned, a scoring rule is triggered at a certain stage as the Reporter is processing the Autocue. Below, we list and explain the seven triggers with Python code. (The complete default Autocue is in &lt;strong&gt;autocues.py&lt;/strong&gt;, which is easily extensible with additional rules.)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;HTML&lt;/strong&gt;, operates on the raw HTML. For example: split a paragraph with two consecutive line breaks into two paragraphs&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RegExReplacer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;&amp;lt;br */? *&amp;gt;[&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;n]\*&amp;lt;br */? *&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PRE_TRAVERSAL&lt;/strong&gt;, scores or prunes (deletes tags) before the DOM is traversed. This is useful for getting rid of specific tags such as footers, or give positive scores to certain tags For example, delete all comments (specific to a certain news property):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CSSSelector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;div#comments&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pruner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;PRE_TRAVERSAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, the HTML will be traversed as explained above.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;EVAL_PARAGRAPH&lt;/strong&gt;, scores a tag as a paragraph. For example, by counting words.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Scorer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RegExMatcher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;(\w)+([&amp;#39;`]\w)?&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;word&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;reset_children&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EVAL_PARAGRAPH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;EVAL_CONTAINER&lt;/strong&gt;, scores a tag as a container. For example, combining the scores of the children tags with a 70 points penalty, giving a minimal score of 0.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScoreAggregator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;EVAL_CONTAINER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This concludes the traversing of the HTML.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;POST_TRAVERSAL&lt;/strong&gt;, scores or prunes tags after Reporter has traversed the HTML. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tag with the highest score is selected as news container.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NEWS_CONTAINER&lt;/strong&gt; is like POST_TRAVERSAL but only applies to the tag that is selected as news container.&lt;/p&gt;

&lt;p&gt;Example: penalize DIVs inside the news container:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CSSSelector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;div&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scorer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FixedValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;NEWS_CONTAINER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Example: Get rid of any tags that have a score below -50:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ScoreSelector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;upper&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pruner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NEWS_CONTAINER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NEWS_TEXT&lt;/strong&gt;, operates on the text inside the news container. For example, put all text on one line:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;default_autocue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RegExReplacer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;\s+&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NEWS_TEXT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, we can return the final text as the main text of the HTML!&lt;/p&gt;

&lt;p&gt;If you like what I had to say then you may want to &lt;a href=&quot;https://twitter.com/jeroenhjanssens&quot;&gt;follow me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 31 Aug 2013 14:00:00 +0200</pubDate>
				<link>http://www.jeroenjanssens.com/2013/08/31/extracting-text-from-html-with-reporter.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/08/31/extracting-text-from-html-with-reporter.html</guid>
			</item>
		
			<item>
				<title>Bayesian headline testing at Visual Revenue</title>
				<description>&lt;p&gt;&lt;em&gt;Note: This post originally appeared on February 4, 2013 on &lt;a href=&quot;http://visualrevenue.com/blog&quot;&gt;Visual Revenue&amp;#39;s blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Visual Revenue platform provides a number of great tools that support editors to optimize their front page. &lt;a href=&quot;http://visualrevenue.com/instant-headline-testing&quot;&gt;Instant Headline Testing&lt;/a&gt; is one of those tools. The quality of a story headline greatly influences its click-through-rate (CTR). Front page editors therefore spend a lot of thought coming up with the right wording to engage their readers. But on digital media, headlines do not have to be set in stone. Instant Headline Testing gives the editor the opportunity to &lt;a href=&quot;http://visualrevenue.com/blog/2012/08/usa-today-sports-boosted-their-olympics-coverage-by-57-with-headline-testing.html&quot;&gt;improve the quality of a headline&lt;/a&gt; after it has made the front page. Let me give you an example.&lt;/p&gt;

&lt;h3&gt;A sporty example&lt;/h3&gt;

&lt;p&gt;With Super Bowl XLVII (and its power outage) still fresh in our minds, one of our clients, &lt;a href=&quot;http://www.usatoday.com/sports/&quot;&gt;USA Today Sports&lt;/a&gt;, used our platform to conduct the following headline test:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Headline A: &amp;quot;What Harbaugh regrets about Super Bowl&amp;quot; (3.06% CTR)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Headline B: &amp;quot;John Harbaugh explains Super Bowl tirade&amp;quot; (4.93% CTR)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Headline A (the original headline) got 3.06% CTR and headline B got 4.93% CTR, which are both strong CTRs. After only seven minutes of testing the two headlines, headline B had been declared winner with 99.93% certainty (explained later). Subsequently, the winning headline was served to 100% of the audience for over one hour. Finally, the editor even made the change permanent in their CMS. Note that by changing a few words only, a 61% lift had been achieved, which eventually resulted in tens of thousands more views for &lt;a href=&quot;http://www.usatoday.com/story/sports/nfl/2013/02/04/ravens-john-harbaugh-super-bowl-jim-harbaugh-49ers/1890387/&quot;&gt;that article&lt;/a&gt;!&lt;/p&gt;

&lt;h3&gt;Four challenges for instant headline testing&lt;/h3&gt;

&lt;p&gt;Instant Headline Testing is essentially &lt;a href=&quot;http://www.alistapart.com/articles/a-primer-on-a-b-testing/&quot;&gt;A/B testing&lt;/a&gt; for story headlines. However, there are four challenges when it comes to Instant Headline Testing that we need to take into account.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Headlines may be on the front page for only a couple of hours, so a headline test cannot take too long.&lt;/li&gt;
&lt;li&gt;The number of readers varies greatly per front page.&lt;/li&gt;
&lt;li&gt;The CTR of a headline depends on where it is positioned on the front page. For example, a headline positioned at the hero spot has a much higher CTR than one positioned at bottom.&lt;/li&gt;
&lt;li&gt;Front pages are dynamic, so headlines can change position.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Frequentist approach to instant headline testing&lt;/h3&gt;

&lt;p&gt;Our improved implementation overcomes these four challenges by using a Bayesian approach. Before I explain that, I’ll first discuss the frequentist approach to Instant Headline Testing.&lt;/p&gt;

&lt;p&gt;To conclude which headline is better than the other, we cannot just look at the highest CTR. We need to apply statistics in order make sure that the difference in CTR did not happen by chance. It may be the case that we cannot declare a headline as winner at all.&lt;/p&gt;

&lt;p&gt;We apply statistics to the data that we have collected during the headline test. This data includes front page impressions (i.e., views) and clicks. The more data the better, right? Well, not quite. It’s important to realize is that the longer a headline test is running, the longer we are serving one headline of possibly lesser quality to 50% of the readers. This means that the corresponding article may lose out on value. In decision theory, the difference between the actual and potential article impressions is known as &lt;a href=&quot;http://en.wikipedia.org/wiki/Regret_(decision_theory)&quot;&gt;regret&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, on the one hand, we want to collect as much data as possible in order to make a reliable conclusion, while on the other hand, we want to maximize article impressions, i.e., minimize regret. This raises two questions: (1) When can we stop a headline test? and (2) How do we know that one headline is better than the other?&lt;/p&gt;

&lt;p&gt;Within statistics, the frequentist approach and the Bayesian approach are two well-known approaches when it comes to A/B testing. The frequentist approach provides a &lt;a href=&quot;http://visualwebsiteoptimizer.com/ab-split-significance-calculator/&quot;&gt;statistical test&lt;/a&gt; whether the CTRs of the two headlines are &lt;a href=&quot;http://en.wikipedia.org/wiki/Statistical_significance&quot;&gt;significantly different&lt;/a&gt;. That would answer our second question.&lt;/p&gt;

&lt;p&gt;The frequentist approach doesn’t provide an easy answer to the first question because the statistical test assumes that the number of views are fixed before we start a headline test. Furthermore, we cannot run a headline test until we see a significant difference between CTRs as this would falsely increase the probability of obtaining a significant result, as &lt;a href=&quot;http://www.evanmiller.org/how-not-to-run-an-ab-test.html&quot;&gt;Evan Mill explains on his blog&lt;/a&gt;. We would have to estimate how many views we would need in order to obtain a significant difference.&lt;/p&gt;

&lt;p&gt;And this is where the four challenges come into play. Due to challenge 1, the headline test cannot take too long, say 20 minutes at most, which limits the number of views we can get. Because of challenge 2, the views per minute may be anything between 10 and 10,000, and we do need to have a tool that’s usable by all our front page editors. Challenge 3 determines the CTRs of the headlines as well. When the CTRs are closer together, we need more views in order to obtain a statistically significant difference. Including these three challenges when estimating the desired number of views is not straightforward. On top of that, when a headline changes position during a test (which is the fourth challenge), our estimate becomes completely invalid!&lt;/p&gt;

&lt;p&gt;Let’s have a look at a Bayesian approach to Instant Headline Testing, which is one that I find much more straightforward and elegant.&lt;/p&gt;

&lt;h3&gt;Bayesian approach to instant headline testing&lt;/h3&gt;

&lt;p&gt;Whereas the frequentist approach assumes that the “true” CTRs remain the same, all that the Bayesian approach cares about is the data we have actually observed. So, we don’t need to worry about estimating the required number of views beforehand. Moreover, the Bayesian approach doesn’t mind that headlines change position while testing, so that overcomes challenge 4.&lt;/p&gt;

&lt;p&gt;Below I’ll first explain how the Bayesian approach determines when to stop a headline test (which was our first question). The second question, determining whether one headline is better than the other is discussed in the next section.&lt;/p&gt;

&lt;p&gt;During the headline test we keep track of number of views and number of clicks on headline A and B. Important here is the absolute difference between the number of clicks both headlines. When the absolute difference crosses the so-called Anscombe boundary, we stop the headline test.&lt;/p&gt;

&lt;p&gt;The Anscombe boundary is defined in terms of (1) the number of views so far, (denoted by &lt;img src=&quot;/img/eq_n.gif&quot; alt=&quot;&quot;&gt;) and (2) the number of views we will be serving the winning headline (denoted by &lt;img src=&quot;/img/eq_k.gif&quot; alt=&quot;&quot;&gt;). Stated more formally, we stop when the following inequality is true:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/eq_anscombe.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;where &lt;img src=&quot;/img/eq_y.gif&quot; alt=&quot;&quot;&gt; is the absolute difference between the number of clicks for both headlines. The shape of the Anscombe boundary is shown in the figure below. Here, we assumed that the front page gets 1,000 views per minute (VPM). So, after 20 minutes of testing, the front page has been viewed 20,000 times. The figure also shows the absolute difference for five simulated headline tests (CTR A=5%, CTR B=3%), which are denoted by gray lines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/vr-ht-anscombe.png&quot; alt=&quot;Anscombe Boundary&quot; title=&quot;Anscombe Boundary&quot;&gt;&lt;/p&gt;

&lt;p&gt;The figure illustrates that the Anscombe boundary takes the trade-off between the number of views so far, and the number of views after conclusion into account. In a recent blog post, Aaron Goodman performed an &lt;a href=&quot;http://blog.custora.com/2012/05/a-bayesian-approach-to-ab-testing/&quot;&gt;interesting comparison&lt;/a&gt; between the frequentist, multiple testing, and Bayesian approaches. He demonstrated that the Bayesian approach is best at minimizing regret, or, in other words, maximizing article views.&lt;/p&gt;

&lt;p&gt;Again, once the absolute difference passes the Anscombe boundary, we are ready to conclude which headline was better.&lt;/p&gt;

&lt;h3&gt;Certainty of conclusion&lt;/h3&gt;

&lt;p&gt;It would nice to know with how much certainty we can conclude that one headline is better than the other.&lt;/p&gt;

&lt;p&gt;The certainty associated with the CTRs of Headlines A and B can be modeled by a beta distribution. The beta distribution can be defined in terms of the number of views and CTR as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/eq_beta.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

&lt;p&gt;The figure below shows how a beta distribution becomes more peaked as the number of views increases (while keeping the CTR at 20%). It illustrates that as we collect more evidence (i.e., views) our uncertainty about the CTR decreases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/vr-ht-beta-views-ctr.png&quot; alt=&quot;Parameterizing a beta distribution with views and CTR&quot; title=&quot;Parameterizing a beta distribution with views and CTR&quot;&gt;&lt;/p&gt;

&lt;p&gt;The gray lines may give the impression that the peakedness increases linearly with the number of views, but please note that these lines represent the number of views on a log scale. Allow me to clarify this with the following image, which shows that the interval in which 95% of the probability density is located, i.e., the 95% credibility interval as it is called in Bayesian statistics, decreases exponentially with respect to the number of views.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/vr-ht-credibility-interval.png&quot; alt=&quot;Credibility Interval&quot; title=&quot;Credibility Interval&quot;&gt;&lt;/p&gt;

&lt;p&gt;The amount of overlap between A’s beta distributions and B’s beta distribution determines the certainty of our conclusion. We can estimate this certainty by generating a random value (i.e., drawing a random sample) from both beta distributions and note which value one is higher. If we repeat this, say, a million times, we can accurately estimate the probability that B is better than A. This probability is the certainty with which we can declare headline B as the true winner.&lt;/p&gt;

&lt;p&gt;Instead of estimating the certainty, it can also be computed in closed form &lt;a href=&quot;http://www.johndcook.com/blog/2012/10/11/beta-inequalities-in-r/&quot;&gt;(see this post from John Cook’s blog)&lt;/a&gt;, but the
&lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html&quot;&gt;stats.beta.pdf function from scipy&lt;/a&gt; doesn’t like very peaked beta distributions. One of the few exceptions that R is better than python! ;-)&lt;/p&gt;

&lt;p&gt;Below are two figures that show how long it takes to conclude and the amount of certainty associated to that conclusion. In the first figure, the CTRs of both headlines are kept constant (CTR A=5%, CTR B=3%), and the views per minute (VPM) varies from 10 to 10,000. In the second figure, the VPM (=1000) and the CTR of headline A (=5%) are kept constant and the CTR of headline B varies from 0% to 10%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/vr-ht-time-to-conclude.png&quot; alt=&quot;Time to conclude&quot; title=&quot;Time to conclude&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/vr-ht-certainty.png&quot; alt=&quot;Time to conclude&quot; title=&quot;Time to conclude&quot;&gt;&lt;/p&gt;

&lt;p&gt;In case of sufficient certainty, we declare the headline with the highest CTR as the winner of the headline test. After that, the editor may decide to continue to serve the winning headline to 100% of the readers.&lt;/p&gt;

&lt;p&gt;The serving aspect is actually taken care of by our platform and doesn’t require any additional integration, but that’s material for a blog post that one of our fine front-end engineers should write!&lt;/p&gt;

&lt;h3&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;Visual Revenue’s Instant Headline Testing tool enables editors to A/B test easily different headlines and to quickly see whether the quality has improved. The frequentist approach, which is often used for A/B tests, is unable to overcome the challenges that are associated with A/B testing headlines. The Bayesian approach, however, offers the flexibility that front pages require.&lt;/p&gt;

&lt;p&gt;I have explained how the Bayesian approach, using the Anscombe boundary, determines when the stop a headline test. I also discussed how we can compute the certainty associated with concluding that one headline is better than the other.&lt;/p&gt;

&lt;p&gt;If you would like to play with beta distributions and compute the associated certainty, Peak Conversion provides a nice &lt;a href=&quot;http://www.peakconversion.com/2012/02/ab-split-test-graphical-calculator/&quot;&gt;graphical calculator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If I still haven’t convinced you that the Bayesian approach is the way to go, then you may want to have a look at: &lt;strong&gt;John A. List, Sally Sadoff, and Mathis Wagner. &lt;em&gt;“So you want to run an experiment, now what? Some Simple Rules of Thumb for Optimal Experimental Design.”&lt;/em&gt; NBER Working Paper No. 15701&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, the frequentist approach provides methods that allow for setting up “checkpoints” where you may determine whether you want to stop a headline test. In other words, these methods offer &lt;a href=&quot;http://en.wikipedia.org/wiki/Multiple_comparisons&quot;&gt;correction for multiple testing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you like what I had to say then you may want to &lt;a href=&quot;https://twitter.com/jeroenhjanssens&quot;&gt;follow me on Twitter&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sun, 18 Aug 2013 12:00:00 +0200</pubDate>
				<link>http://www.jeroenjanssens.com/2013/08/18/bayesian-headline-testing-at-visual-revenue.html</link>
				<guid isPermaLink="true">http://www.jeroenjanssens.com/2013/08/18/bayesian-headline-testing-at-visual-revenue.html</guid>
			</item>
		
	</channel>
</rss>
